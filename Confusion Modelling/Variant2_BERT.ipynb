{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XVYAOrOzwkWD"
   },
   "source": [
    "#  Variant 2 - BERT - Tensor flow 2  and High Memory CPU Instance of 14GB RAM required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "igsMP5y872E6",
    "outputId": "f36e39c6-b599-4757-dfe1-c8bde3de2421"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow                    2.3.0          \n",
      "tensorflow-addons             0.8.3          \n",
      "tensorflow-datasets           2.1.0          \n",
      "tensorflow-estimator          2.3.0          \n",
      "tensorflow-gcs-config         2.3.0          \n",
      "tensorflow-hub                0.9.0          \n",
      "tensorflow-metadata           0.23.0         \n",
      "tensorflow-privacy            0.2.2          \n",
      "tensorflow-probability        0.11.0         \n",
      "transformers                  3.1.0          \n"
     ]
    }
   ],
   "source": [
    "# software needed - Tensorflow 2\n",
    "! pip list | grep \"tensorflow\"   # Check tensorflow==2.0.0, tensorflow-gpu==2.0.0\n",
    "! pip list | grep \"transformers\" # Check transformers>=2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "colab_type": "code",
    "id": "yNZxOgWq-DP4",
    "outputId": "aa0f94bd-0fc4-436b-c693-fb586ef47920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandasql in /usr/local/lib/python3.6/dist-packages (0.7.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pandasql) (1.0.5)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pandasql) (1.18.5)\n",
      "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.6/dist-packages (from pandasql) (1.3.19)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->pandasql) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->pandasql) (2018.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->pandasql) (1.15.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "# imports and installs\n",
    "!pip install pandasql\n",
    "!pip install transformers\n",
    "import pandasql as ps\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense, SpatialDropout1D,Dropout, Activation, Flatten, Embedding, Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, LSTM, Activation,SimpleRNN,concatenate,Flatten,MaxPooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "pgUT3LKpIGD-",
    "outputId": "2f71fc37-3edf-463d-9c3b-8c35a423e29d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Dataset load and headers\n",
    "X_Y_full = pd.read_excel('confusion_excel_800.xlsx', sheet_name='Sheet1')\n",
    "del X_Y_full['Unnamed: 1']\n",
    "del X_Y_full['Unnamed: 3']\n",
    "del X_Y_full['Unnamed: 5']\n",
    "X_Y_full.columns = ['request','response','confusion']\n",
    "X_Y_full = X_Y_full[X_Y_full['request'].notnull()]\n",
    "X_Y_full = X_Y_full[X_Y_full['response'].notnull()]\n",
    "\n",
    "#Lemmatization\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    listToStr = ' '.join(map(str, [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]))\n",
    "    return listToStr\n",
    "X_Y_full['request'] = X_Y_full['request'].apply(lemmatize_text)\n",
    "X_Y_full['response'] = X_Y_full['response'].apply(lemmatize_text)\n",
    "\n",
    "# Stop words Removal\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "X_Y_full = X_Y_full[X_Y_full['request'].notnull()]\n",
    "X_Y_full = X_Y_full[X_Y_full['response'].notnull()]\n",
    "X_Y_full['request'] = X_Y_full['request'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "X_Y_full['response'] = X_Y_full['response'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "X_Y_full['request']  = X_Y_full['request'].str.lower().str.strip()\n",
    "X_Y_full['response']  = X_Y_full['response'].str.lower().str.strip()\n",
    "X_Y_full['confusion'] = X_Y_full['confusion'].str.lower().str.strip()\n",
    "\n",
    "\n",
    "# Data Splitting\n",
    "train, validate,test = np.split(X_Y_full.sample(frac=1).reset_index(drop=True), [int(.8*len(X_Y_full)), int(0.9*len(X_Y_full))])\n",
    "x_train_request = (train.request)\n",
    "x_train_response = (train.response)\n",
    "y_train = (train.confusion)\n",
    "x_validate_request = (validate.request)\n",
    "x_validate_response = (validate.response)\n",
    "y_validate = (validate.confusion)\n",
    "x_test_request = (test.request )\n",
    "x_test_response = (test.response )\n",
    "y_test = (test.confusion)\n",
    "\n",
    "\n",
    "#Convert target to 0 and 1 and do numpy transformations to leverage fast parallel processing\n",
    "y_train_new = ps.sqldf(\"select case when lower(confusion)='yes' then 1 else 0 end as confusion_new from train\")\n",
    "y_test_new = ps.sqldf(\"select case when lower(confusion)='yes' then 1 else 0 end as confusion_new from test\")\n",
    "y_validation_new = ps.sqldf(\"select case when lower(confusion)='yes' then 1 else 0 end as confusion_new from validate\")\n",
    "y_train_new = np.asarray(y_train_new['confusion_new'])\n",
    "y_test_new = np.asarray(y_test_new['confusion_new'])\n",
    "y_validation_new = np.asarray(y_validation_new['confusion_new'])\n",
    "y_full = ps.sqldf(\"select case when lower(confusion)='yes' then 1 else 0 end as confusion_new from X_Y_full\")\n",
    "act_y = np.asarray(y_full['confusion_new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "s865NYUj_fbn",
    "outputId": "afc1dbc1-b489-4128-f9af-0f211f2f5944"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "from transformers import TFBertModel\n",
    "\n",
    "#Initialising BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Encode Train set via BERT\n",
    "row = 0\n",
    "list_res = list(x_train_response)\n",
    "req_res_array = np.empty((0,768), float)\n",
    "for j in x_train_request:\n",
    "    #req = encode([j])[0]\n",
    "    input_ids = tf.constant(tokenizer.encode(j))[None, :]  # Batch size 1\n",
    "    outputs = model(input_ids)\n",
    "    req = outputs[1][0]   \n",
    "    input_ids_1 = tf.constant(tokenizer.encode(list_res[row]))[None, :]  # Batch size 1\n",
    "    outputs_1 = model(input_ids_1)\n",
    "    #res = encode([list_res[row]])[0]\n",
    "    res = outputs_1[1][0]  \n",
    "    dif = np.asarray((req - res))\n",
    "    #print(dif)\n",
    "    distance = np.power(dif,2)\n",
    "    \n",
    "    XNormed = distance/np.linalg.norm(distance)\n",
    "    #print(XNormed)\n",
    "    #to_append = np. array(distance)\n",
    "    req_res_array = np.append(req_res_array, [np.asarray(XNormed)],axis=0)\n",
    "    row = row+1\n",
    "    \n",
    "# Encode validation set via BERT    \n",
    "row = 0\n",
    "#import Math\n",
    "list_res = list(x_validate_response)\n",
    "req_valid_array = np.empty((0,768), float)\n",
    "for j in x_validate_request:\n",
    "    #req = encode([j])[0]\n",
    "    input_ids = tf.constant(tokenizer.encode(j))[None, :]  # Batch size 1\n",
    "    outputs = model(input_ids)\n",
    "    req = outputs[1][0]   \n",
    "    input_ids_1 = tf.constant(tokenizer.encode(list_res[row]))[None, :]  # Batch size 1\n",
    "    outputs_1 = model(input_ids_1)\n",
    "    #res = encode([list_res[row]])[0]\n",
    "    res = outputs_1[1][0]  \n",
    "    dif = np.asarray((req - res))\n",
    "    #print(dif)\n",
    "    distance = np.power(dif,2)\n",
    "    XNormed = distance/np.linalg.norm(distance)\n",
    "    #print(XNormed)\n",
    "    #to_append = np. array(distance)\n",
    "    req_valid_array = np.append(req_valid_array, [np.asarray(XNormed)],axis=0)\n",
    "    row = row+1\n",
    "\n",
    "# Encode Test set via BERT\n",
    "    \n",
    "row = 0\n",
    "#import Math\n",
    "list_res = list(x_test_response)\n",
    "req_test_array = np.empty((0,768), float)\n",
    "for j in x_test_request:\n",
    "    #req = encode([j])[0]\n",
    "    input_ids = tf.constant(tokenizer.encode(j))[None, :]  # Batch size 1\n",
    "    outputs = model(input_ids)\n",
    "    req = outputs[1][0]   \n",
    "    input_ids_1 = tf.constant(tokenizer.encode(list_res[row]))[None, :]  # Batch size 1\n",
    "    outputs_1 = model(input_ids_1)\n",
    "    #res = encode([list_res[row]])[0]\n",
    "    res = outputs_1[1][0]  \n",
    "    dif = np.asarray((req - res))\n",
    "    #print(dif)\n",
    "    distance = np.power(dif,2)\n",
    "    \n",
    "    XNormed = distance/np.linalg.norm(distance)\n",
    "    #print(XNormed)\n",
    "    #to_append = np. array(distance)\n",
    "    req_test_array = np.append(req_test_array, [np.asarray(XNormed)],axis=0)\n",
    "    row = row+1\n",
    "\n",
    "req_res_array_valid = req_valid_array\n",
    "req_res_array_test = req_test_array \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uLqLukF3GluY"
   },
   "source": [
    "# Modelling Variant 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zh1DLeKvPjOr",
    "outputId": "834375c4-5ef3-4039-e3d2-a2aca717533a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "65/65 [==============================] - 1s 8ms/step - loss: 1.6786 - recall: 0.3003 - precision: 0.5200 - accuracy: 0.5382 - val_loss: 1.0663 - val_recall: 0.0256 - val_precision: 0.3333 - val_accuracy: 0.5000\n",
      "Epoch 2/60\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.8627 - recall: 0.5842 - precision: 0.5446 - accuracy: 0.5725 - val_loss: 0.7464 - val_recall: 0.2821 - val_precision: 0.6471 - val_accuracy: 0.5750\n",
      "Epoch 3/60\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.7138 - recall: 0.3927 - precision: 0.5459 - accuracy: 0.5585 - val_loss: 0.6946 - val_recall: 0.4615 - val_precision: 0.6429 - val_accuracy: 0.6125\n",
      "Epoch 4/60\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.6769 - recall: 0.6304 - precision: 0.5932 - accuracy: 0.6209 - val_loss: 0.7006 - val_recall: 0.1538 - val_precision: 0.4615 - val_accuracy: 0.5000\n",
      "Epoch 5/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.6617 - recall: 0.5380 - precision: 0.6468 - accuracy: 0.6427 - val_loss: 0.7009 - val_recall: 0.5128 - val_precision: 0.5556 - val_accuracy: 0.5625\n",
      "Epoch 6/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.6493 - recall: 0.5347 - precision: 0.6778 - accuracy: 0.6599 - val_loss: 0.7153 - val_recall: 0.8718 - val_precision: 0.5397 - val_accuracy: 0.5750\n",
      "Epoch 7/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.6248 - recall: 0.6106 - precision: 0.7061 - accuracy: 0.6958 - val_loss: 0.7240 - val_recall: 0.7179 - val_precision: 0.5714 - val_accuracy: 0.6000\n",
      "Epoch 8/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.6148 - recall: 0.6040 - precision: 0.6906 - accuracy: 0.6849 - val_loss: 0.7439 - val_recall: 0.6923 - val_precision: 0.5745 - val_accuracy: 0.6000\n",
      "Epoch 9/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.6040 - recall: 0.6337 - precision: 0.6931 - accuracy: 0.6942 - val_loss: 0.7340 - val_recall: 0.4359 - val_precision: 0.5312 - val_accuracy: 0.5375\n",
      "Epoch 10/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.5692 - recall: 0.6832 - precision: 0.7238 - accuracy: 0.7270 - val_loss: 0.7880 - val_recall: 0.2564 - val_precision: 0.5000 - val_accuracy: 0.5125\n",
      "Epoch 11/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.5530 - recall: 0.6898 - precision: 0.7518 - accuracy: 0.7457 - val_loss: 0.7749 - val_recall: 0.4359 - val_precision: 0.4722 - val_accuracy: 0.4875\n",
      "Epoch 12/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.5181 - recall: 0.7195 - precision: 0.7927 - accuracy: 0.7785 - val_loss: 0.7859 - val_recall: 0.5641 - val_precision: 0.5116 - val_accuracy: 0.5250\n",
      "Epoch 13/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.5398 - recall: 0.6865 - precision: 0.7909 - accuracy: 0.7660 - val_loss: 0.7983 - val_recall: 0.3846 - val_precision: 0.5357 - val_accuracy: 0.5375\n",
      "Epoch 14/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.4807 - recall: 0.7525 - precision: 0.8413 - accuracy: 0.8159 - val_loss: 0.8443 - val_recall: 0.5128 - val_precision: 0.5000 - val_accuracy: 0.5125\n",
      "Epoch 15/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.4964 - recall: 0.7327 - precision: 0.8346 - accuracy: 0.8050 - val_loss: 0.8782 - val_recall: 0.4103 - val_precision: 0.5517 - val_accuracy: 0.5500\n",
      "Epoch 16/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.4612 - recall: 0.7558 - precision: 0.8388 - accuracy: 0.8159 - val_loss: 0.9024 - val_recall: 0.6667 - val_precision: 0.5200 - val_accuracy: 0.5375\n",
      "Epoch 17/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.4338 - recall: 0.8053 - precision: 0.8592 - accuracy: 0.8456 - val_loss: 0.9184 - val_recall: 0.4359 - val_precision: 0.5312 - val_accuracy: 0.5375\n",
      "Epoch 18/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.4583 - recall: 0.7525 - precision: 0.8352 - accuracy: 0.8128 - val_loss: 0.8615 - val_recall: 0.5385 - val_precision: 0.4773 - val_accuracy: 0.4875\n",
      "Epoch 19/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.4058 - recall: 0.8284 - precision: 0.9029 - accuracy: 0.8768 - val_loss: 0.9883 - val_recall: 0.3077 - val_precision: 0.5000 - val_accuracy: 0.5125\n",
      "Epoch 20/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.3917 - recall: 0.8119 - precision: 0.8978 - accuracy: 0.8674 - val_loss: 0.9378 - val_recall: 0.4103 - val_precision: 0.5926 - val_accuracy: 0.5750\n",
      "Epoch 21/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.3927 - recall: 0.8119 - precision: 0.8786 - accuracy: 0.8580 - val_loss: 0.9420 - val_recall: 0.4615 - val_precision: 0.4865 - val_accuracy: 0.5000\n",
      "Epoch 22/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.3857 - recall: 0.8119 - precision: 0.8754 - accuracy: 0.8565 - val_loss: 0.9753 - val_recall: 0.4103 - val_precision: 0.5333 - val_accuracy: 0.5375\n",
      "Epoch 23/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.3752 - recall: 0.8218 - precision: 0.9055 - accuracy: 0.8752 - val_loss: 1.0325 - val_recall: 0.5128 - val_precision: 0.5128 - val_accuracy: 0.5250\n",
      "Epoch 24/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.3338 - recall: 0.8548 - precision: 0.9024 - accuracy: 0.8877 - val_loss: 1.0503 - val_recall: 0.3077 - val_precision: 0.5000 - val_accuracy: 0.5125\n",
      "Epoch 25/60\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.3345 - recall: 0.8581 - precision: 0.8997 - accuracy: 0.8877 - val_loss: 1.0796 - val_recall: 0.3333 - val_precision: 0.5417 - val_accuracy: 0.5375\n",
      "Epoch 26/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.3318 - recall: 0.8581 - precision: 0.9187 - accuracy: 0.8970 - val_loss: 1.1181 - val_recall: 0.6410 - val_precision: 0.5319 - val_accuracy: 0.5500\n",
      "Epoch 27/60\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.3191 - recall: 0.8812 - precision: 0.9020 - accuracy: 0.8986 - val_loss: 1.0819 - val_recall: 0.4103 - val_precision: 0.5161 - val_accuracy: 0.5250\n",
      "Epoch 28/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.2917 - recall: 0.8713 - precision: 0.9429 - accuracy: 0.9142 - val_loss: 1.1538 - val_recall: 0.3590 - val_precision: 0.5385 - val_accuracy: 0.5375\n",
      "Epoch 29/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.3014 - recall: 0.8614 - precision: 0.8969 - accuracy: 0.8877 - val_loss: 1.1516 - val_recall: 0.5641 - val_precision: 0.5366 - val_accuracy: 0.5500\n",
      "Epoch 30/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.2751 - recall: 0.8944 - precision: 0.9281 - accuracy: 0.9173 - val_loss: 1.1985 - val_recall: 0.4103 - val_precision: 0.5000 - val_accuracy: 0.5125\n",
      "Epoch 31/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.2820 - recall: 0.8812 - precision: 0.9303 - accuracy: 0.9126 - val_loss: 1.2072 - val_recall: 0.3077 - val_precision: 0.6000 - val_accuracy: 0.5625\n",
      "Epoch 32/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.3007 - recall: 0.8680 - precision: 0.9132 - accuracy: 0.8986 - val_loss: 1.1965 - val_recall: 0.5641 - val_precision: 0.5366 - val_accuracy: 0.5500\n",
      "Epoch 33/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.2915 - recall: 0.8548 - precision: 0.9384 - accuracy: 0.9048 - val_loss: 1.2711 - val_recall: 0.6410 - val_precision: 0.4902 - val_accuracy: 0.5000\n",
      "Epoch 34/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.2837 - recall: 0.8911 - precision: 0.8882 - accuracy: 0.8955 - val_loss: 1.2287 - val_recall: 0.3333 - val_precision: 0.5909 - val_accuracy: 0.5625\n",
      "Epoch 35/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.2681 - recall: 0.9076 - precision: 0.9483 - accuracy: 0.9329 - val_loss: 1.2871 - val_recall: 0.3333 - val_precision: 0.4815 - val_accuracy: 0.5000\n",
      "Epoch 36/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.2218 - recall: 0.9208 - precision: 0.9755 - accuracy: 0.9516 - val_loss: 1.3542 - val_recall: 0.5897 - val_precision: 0.5111 - val_accuracy: 0.5250\n",
      "Epoch 37/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.2342 - recall: 0.9274 - precision: 0.9493 - accuracy: 0.9423 - val_loss: 1.3491 - val_recall: 0.2821 - val_precision: 0.5500 - val_accuracy: 0.5375\n",
      "Epoch 38/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1999 - recall: 0.9373 - precision: 0.9726 - accuracy: 0.9579 - val_loss: 1.3539 - val_recall: 0.4872 - val_precision: 0.5429 - val_accuracy: 0.5500\n",
      "Epoch 39/60\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.2079 - recall: 0.9208 - precision: 0.9654 - accuracy: 0.9470 - val_loss: 1.4270 - val_recall: 0.5897 - val_precision: 0.5000 - val_accuracy: 0.5125\n",
      "Epoch 40/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.2354 - recall: 0.9307 - precision: 0.9216 - accuracy: 0.9298 - val_loss: 1.4289 - val_recall: 0.4872 - val_precision: 0.5278 - val_accuracy: 0.5375\n",
      "Epoch 41/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.2199 - recall: 0.9175 - precision: 0.9456 - accuracy: 0.9360 - val_loss: 1.4100 - val_recall: 0.5385 - val_precision: 0.5833 - val_accuracy: 0.5875\n",
      "Epoch 42/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1856 - recall: 0.9439 - precision: 0.9662 - accuracy: 0.9579 - val_loss: 1.4876 - val_recall: 0.5385 - val_precision: 0.5526 - val_accuracy: 0.5625\n",
      "Epoch 43/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1652 - recall: 0.9472 - precision: 0.9729 - accuracy: 0.9626 - val_loss: 1.4960 - val_recall: 0.4359 - val_precision: 0.5484 - val_accuracy: 0.5500\n",
      "Epoch 44/60\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.1634 - recall: 0.9505 - precision: 0.9829 - accuracy: 0.9688 - val_loss: 1.5357 - val_recall: 0.5897 - val_precision: 0.5349 - val_accuracy: 0.5500\n",
      "Epoch 45/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1523 - recall: 0.9736 - precision: 0.9801 - accuracy: 0.9782 - val_loss: 1.5770 - val_recall: 0.4615 - val_precision: 0.5625 - val_accuracy: 0.5625\n",
      "Epoch 46/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1609 - recall: 0.9571 - precision: 0.9864 - accuracy: 0.9735 - val_loss: 1.5686 - val_recall: 0.5641 - val_precision: 0.5500 - val_accuracy: 0.5625\n",
      "Epoch 47/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1385 - recall: 0.9670 - precision: 0.9865 - accuracy: 0.9782 - val_loss: 1.5615 - val_recall: 0.5385 - val_precision: 0.5122 - val_accuracy: 0.5250\n",
      "Epoch 48/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1499 - recall: 0.9571 - precision: 0.9699 - accuracy: 0.9657 - val_loss: 1.6311 - val_recall: 0.5641 - val_precision: 0.5366 - val_accuracy: 0.5500\n",
      "Epoch 49/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1282 - recall: 0.9769 - precision: 0.9933 - accuracy: 0.9860 - val_loss: 1.6602 - val_recall: 0.5897 - val_precision: 0.5111 - val_accuracy: 0.5250\n",
      "Epoch 50/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1422 - recall: 0.9538 - precision: 0.9764 - accuracy: 0.9672 - val_loss: 1.6358 - val_recall: 0.4872 - val_precision: 0.5429 - val_accuracy: 0.5500\n",
      "Epoch 51/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1339 - recall: 0.9703 - precision: 0.9767 - accuracy: 0.9750 - val_loss: 1.7660 - val_recall: 0.3333 - val_precision: 0.5417 - val_accuracy: 0.5375\n",
      "Epoch 52/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1311 - recall: 0.9637 - precision: 0.9898 - accuracy: 0.9782 - val_loss: 1.6358 - val_recall: 0.5641 - val_precision: 0.5641 - val_accuracy: 0.5750\n",
      "Epoch 53/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1185 - recall: 0.9670 - precision: 0.9899 - accuracy: 0.9797 - val_loss: 1.7638 - val_recall: 0.5897 - val_precision: 0.5000 - val_accuracy: 0.5125\n",
      "Epoch 54/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1124 - recall: 0.9703 - precision: 0.9899 - accuracy: 0.9813 - val_loss: 1.7157 - val_recall: 0.5641 - val_precision: 0.5641 - val_accuracy: 0.5750\n",
      "Epoch 55/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1081 - recall: 0.9769 - precision: 0.9801 - accuracy: 0.9797 - val_loss: 1.7273 - val_recall: 0.5385 - val_precision: 0.5526 - val_accuracy: 0.5625\n",
      "Epoch 56/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1298 - recall: 0.9670 - precision: 0.9767 - accuracy: 0.9735 - val_loss: 1.7033 - val_recall: 0.5385 - val_precision: 0.5833 - val_accuracy: 0.5875\n",
      "Epoch 57/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1302 - recall: 0.9604 - precision: 0.9864 - accuracy: 0.9750 - val_loss: 1.8967 - val_recall: 0.6154 - val_precision: 0.5000 - val_accuracy: 0.5125\n",
      "Epoch 58/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1013 - recall: 0.9868 - precision: 0.9967 - accuracy: 0.9922 - val_loss: 1.8260 - val_recall: 0.5641 - val_precision: 0.5641 - val_accuracy: 0.5750\n",
      "Epoch 59/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0972 - recall: 0.9868 - precision: 0.9934 - accuracy: 0.9906 - val_loss: 1.7841 - val_recall: 0.4103 - val_precision: 0.5517 - val_accuracy: 0.5500\n",
      "Epoch 60/60\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.0867 - recall: 0.9868 - precision: 1.0000 - accuracy: 0.9938 - val_loss: 1.7591 - val_recall: 0.5128 - val_precision: 0.5556 - val_accuracy: 0.5625\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#model.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(300,input_dim=768,activation='relu'))\n",
    "#model.add(Dropout(rate=0.3))\n",
    "model.add(Dense(20,activation='relu',kernel_regularizer=l2(0.05), bias_regularizer=l2(0.01)))\n",
    "model.add(Dense(10,activation='relu'))\n",
    "#model.add(Dropout(rate=0.4))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "#model.summary()\n",
    "\n",
    "#Loss Function # tried different values for learning_rate\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow import keras\n",
    "AD = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(optimizer=AD,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[keras.metrics.Recall(),keras.metrics.Precision(),'accuracy'])\n",
    "\n",
    "#Training LSTM model\n",
    "hist = model.fit(req_res_array, y_train_new,\n",
    "          batch_size=10,\n",
    "          epochs=60,verbose=1,validation_data=(req_res_array_valid, y_validation_new)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 674
    },
    "colab_type": "code",
    "id": "IbLHAnYrXD2m",
    "outputId": "ef80dfee-908e-4058-b2e1-30ee0ff1efd3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 4ms/step - loss: 1.7591 - recall: 0.5128 - precision: 0.5556 - accuracy: 0.5625\n",
      "0.5128205418586731\n",
      "0.5625\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 1.3165 - recall: 0.5000 - precision: 0.6562 - accuracy: 0.6049\n",
      "0.5\n",
      "0.604938268661499\n"
     ]
    }
   ],
   "source": [
    "# Training and validation accuracy and recall growth through epochs\n",
    "\n",
    "loss_train = hist.history[model.metrics_names[3]]\n",
    "loss_val = hist.history['val_'+model.metrics_names[3]]\n",
    "epochs = range(0,60)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training accuracy')\n",
    "plt.plot(epochs, loss_val, 'b', label='validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "loss_train = hist.history[model.metrics_names[1]]\n",
    "loss_val = hist.history['val_'+model.metrics_names[1]]\n",
    "epochs = range(0,60)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training recall')\n",
    "plt.plot(epochs, loss_val, 'b', label='validation recall')\n",
    "plt.title('Training and Validation Recall')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Recall')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "loss, recall,precision,acc = model.evaluate(req_res_array_valid, y_validation_new)\n",
    "print(recall)\n",
    "print(acc)\n",
    "\n",
    "loss,recall,precision, acc = model.evaluate(req_res_array_test, y_test_new)\n",
    "print(recall)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EQMf71JwGlud"
   },
   "source": [
    "# Evaluation of Variant 2 - 10 fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHQ_FQ09RT8U"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "num_folds = 10\n",
    "\n",
    "# Define per-fold score containers <-- these are new\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "rec_per_fold = []\n",
    "per_per_fold = []\n",
    "acc_per_fold_HS=[]\n",
    "rec_per_fold_HS=[]\n",
    "inputs = np.concatenate((req_res_array, req_res_array_valid, req_res_array_test), axis=0)\n",
    "targets = np.concatenate((y_train_new, y_validation_new,y_test_new), axis=0)\n",
    "#HS_ = np.concatenate((train_HS_new, validate_HS_new,test_HS_new), axis=0)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jwMafTpTRbNY",
    "outputId": "07913da9-6020-451c-df9d-4d49c03d900c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6174 - recall: 0.0774 - accuracy: 0.5229 - precision: 0.5510\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.8375 - recall: 0.2636 - accuracy: 0.5479 - precision: 0.5714\n",
      "Epoch 3/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.7075 - recall: 0.2607 - accuracy: 0.5659 - precision: 0.6233\n",
      "Epoch 4/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6876 - recall: 0.4097 - accuracy: 0.6033 - precision: 0.6413\n",
      "Epoch 5/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6817 - recall: 0.6132 - accuracy: 0.5964 - precision: 0.5784\n",
      "Epoch 6/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6653 - recall: 0.5817 - accuracy: 0.6602 - precision: 0.6722\n",
      "Epoch 7/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6541 - recall: 0.5444 - accuracy: 0.6588 - precision: 0.6859\n",
      "Epoch 8/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6442 - recall: 0.5931 - accuracy: 0.6574 - precision: 0.6635\n",
      "Epoch 9/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6308 - recall: 0.6476 - accuracy: 0.6810 - precision: 0.6787\n",
      "Epoch 10/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6092 - recall: 0.6877 - accuracy: 0.7060 - precision: 0.6997\n",
      "Epoch 11/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6040 - recall: 0.7020 - accuracy: 0.7295 - precision: 0.7292\n",
      "Epoch 12/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6004 - recall: 0.6648 - accuracy: 0.7074 - precision: 0.7117\n",
      "Epoch 13/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5687 - recall: 0.6963 - accuracy: 0.7406 - precision: 0.7500\n",
      "Epoch 14/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5588 - recall: 0.7307 - accuracy: 0.7379 - precision: 0.7286\n",
      "Epoch 15/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5286 - recall: 0.7479 - accuracy: 0.7698 - precision: 0.7699\n",
      "Epoch 16/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5209 - recall: 0.7335 - accuracy: 0.7559 - precision: 0.7552\n",
      "Epoch 17/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5090 - recall: 0.7765 - accuracy: 0.7850 - precision: 0.7787\n",
      "Epoch 18/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4958 - recall: 0.7593 - accuracy: 0.7836 - precision: 0.7864\n",
      "Epoch 19/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4661 - recall: 0.8023 - accuracy: 0.8252 - precision: 0.8309\n",
      "Epoch 20/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4415 - recall: 0.7937 - accuracy: 0.8211 - precision: 0.8293\n",
      "Epoch 21/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4251 - recall: 0.8195 - accuracy: 0.8322 - precision: 0.8314\n",
      "Epoch 22/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4344 - recall: 0.7822 - accuracy: 0.8252 - precision: 0.8452\n",
      "Epoch 23/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4035 - recall: 0.8223 - accuracy: 0.8474 - precision: 0.8567\n",
      "Epoch 24/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3898 - recall: 0.8453 - accuracy: 0.8460 - precision: 0.8381\n",
      "Epoch 25/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3886 - recall: 0.8625 - accuracy: 0.8613 - precision: 0.8527\n",
      "Epoch 26/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3516 - recall: 0.8768 - accuracy: 0.8779 - precision: 0.8718\n",
      "Epoch 27/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3335 - recall: 0.8625 - accuracy: 0.8821 - precision: 0.8905\n",
      "Epoch 28/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3539 - recall: 0.8854 - accuracy: 0.8738 - precision: 0.8583\n",
      "Epoch 29/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3126 - recall: 0.8653 - accuracy: 0.8918 - precision: 0.9069\n",
      "Epoch 30/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2914 - recall: 0.8911 - accuracy: 0.9098 - precision: 0.9201\n",
      "Epoch 31/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2793 - recall: 0.8997 - accuracy: 0.9182 - precision: 0.9290\n",
      "Epoch 32/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2692 - recall: 0.9026 - accuracy: 0.9196 - precision: 0.9292\n",
      "Epoch 33/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3033 - recall: 0.9083 - accuracy: 0.9071 - precision: 0.9006\n",
      "Epoch 34/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2422 - recall: 0.9083 - accuracy: 0.9293 - precision: 0.9435\n",
      "Epoch 35/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2404 - recall: 0.9169 - accuracy: 0.9223 - precision: 0.9222\n",
      "Epoch 36/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2198 - recall: 0.9284 - accuracy: 0.9362 - precision: 0.9391\n",
      "Epoch 37/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2151 - recall: 0.9312 - accuracy: 0.9390 - precision: 0.9420\n",
      "Epoch 38/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2183 - recall: 0.9140 - accuracy: 0.9293 - precision: 0.9382\n",
      "Epoch 39/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2069 - recall: 0.9312 - accuracy: 0.9431 - precision: 0.9503\n",
      "Epoch 40/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2545 - recall: 0.9083 - accuracy: 0.9196 - precision: 0.9242\n",
      "Epoch 41/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1881 - recall: 0.9398 - accuracy: 0.9556 - precision: 0.9676\n",
      "Epoch 42/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1757 - recall: 0.9542 - accuracy: 0.9598 - precision: 0.9624\n",
      "Epoch 43/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1756 - recall: 0.9513 - accuracy: 0.9584 - precision: 0.9623\n",
      "Epoch 44/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1951 - recall: 0.9427 - accuracy: 0.9445 - precision: 0.9427\n",
      "Epoch 45/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1688 - recall: 0.9599 - accuracy: 0.9626 - precision: 0.9626\n",
      "Epoch 46/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1647 - recall: 0.9656 - accuracy: 0.9667 - precision: 0.9656\n",
      "Epoch 47/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1766 - recall: 0.9542 - accuracy: 0.9584 - precision: 0.9597\n",
      "Epoch 48/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1556 - recall: 0.9599 - accuracy: 0.9667 - precision: 0.9710\n",
      "Epoch 49/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1512 - recall: 0.9542 - accuracy: 0.9639 - precision: 0.9708\n",
      "Epoch 50/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1263 - recall: 0.9685 - accuracy: 0.9778 - precision: 0.9854\n",
      "Epoch 51/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1371 - recall: 0.9771 - accuracy: 0.9764 - precision: 0.9743\n",
      "Epoch 52/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1531 - recall: 0.9570 - accuracy: 0.9653 - precision: 0.9709\n",
      "Epoch 53/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1268 - recall: 0.9771 - accuracy: 0.9764 - precision: 0.9743\n",
      "Epoch 54/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1256 - recall: 0.9685 - accuracy: 0.9723 - precision: 0.9741\n",
      "Epoch 55/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1080 - recall: 0.9771 - accuracy: 0.9778 - precision: 0.9771\n",
      "Epoch 56/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1170 - recall: 0.9713 - accuracy: 0.9764 - precision: 0.9798\n",
      "Epoch 57/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1834 - recall: 0.9398 - accuracy: 0.9515 - precision: 0.9591\n",
      "Epoch 58/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1208 - recall: 0.9771 - accuracy: 0.9820 - precision: 0.9855\n",
      "Epoch 59/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1076 - recall: 0.9885 - accuracy: 0.9834 - precision: 0.9773\n",
      "Epoch 60/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1185 - recall: 0.9742 - accuracy: 0.9736 - precision: 0.9714\n",
      "Score for fold 1: loss of 1.1780860424041748; recall of 71.42857313156128% ; accuracy of 69.13580298423767%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6003 - recall_1: 0.9114 - accuracy: 0.5090 - precision_1: 0.4969\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.8358 - recall_1: 0.2857 - accuracy: 0.5631 - precision_1: 0.6061\n",
      "Epoch 3/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.7073 - recall_1: 0.3171 - accuracy: 0.5700 - precision_1: 0.6099\n",
      "Epoch 4/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6939 - recall_1: 0.4829 - accuracy: 0.5437 - precision_1: 0.5331\n",
      "Epoch 5/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6882 - recall_1: 0.6114 - accuracy: 0.5895 - precision_1: 0.5722\n",
      "Epoch 6/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6867 - recall_1: 0.6086 - accuracy: 0.5992 - precision_1: 0.5836\n",
      "Epoch 7/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6735 - recall_1: 0.5543 - accuracy: 0.6297 - precision_1: 0.6361\n",
      "Epoch 8/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6637 - recall_1: 0.6000 - accuracy: 0.6366 - precision_1: 0.6325\n",
      "Epoch 9/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6520 - recall_1: 0.6343 - accuracy: 0.6560 - precision_1: 0.6491\n",
      "Epoch 10/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6487 - recall_1: 0.5971 - accuracy: 0.6574 - precision_1: 0.6635\n",
      "Epoch 11/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6297 - recall_1: 0.6143 - accuracy: 0.6824 - precision_1: 0.6958\n",
      "Epoch 12/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6216 - recall_1: 0.5857 - accuracy: 0.6768 - precision_1: 0.6997\n",
      "Epoch 13/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6141 - recall_1: 0.6571 - accuracy: 0.6907 - precision_1: 0.6907\n",
      "Epoch 14/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5920 - recall_1: 0.6857 - accuracy: 0.7212 - precision_1: 0.7251\n",
      "Epoch 15/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5870 - recall_1: 0.6457 - accuracy: 0.7268 - precision_1: 0.7559\n",
      "Epoch 16/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5794 - recall_1: 0.6800 - accuracy: 0.7240 - precision_1: 0.7323\n",
      "Epoch 17/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5560 - recall_1: 0.7086 - accuracy: 0.7490 - precision_1: 0.7584\n",
      "Epoch 18/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5552 - recall_1: 0.7029 - accuracy: 0.7393 - precision_1: 0.7455\n",
      "Epoch 19/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5336 - recall_1: 0.7343 - accuracy: 0.7531 - precision_1: 0.7515\n",
      "Epoch 20/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5238 - recall_1: 0.7029 - accuracy: 0.7656 - precision_1: 0.7910\n",
      "Epoch 21/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5061 - recall_1: 0.7286 - accuracy: 0.7767 - precision_1: 0.7944\n",
      "Epoch 22/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4825 - recall_1: 0.7800 - accuracy: 0.7975 - precision_1: 0.7982\n",
      "Epoch 23/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4710 - recall_1: 0.7571 - accuracy: 0.8086 - precision_1: 0.8333\n",
      "Epoch 24/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4576 - recall_1: 0.8086 - accuracy: 0.8225 - precision_1: 0.8227\n",
      "Epoch 25/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4592 - recall_1: 0.7800 - accuracy: 0.8128 - precision_1: 0.8248\n",
      "Epoch 26/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4323 - recall_1: 0.7829 - accuracy: 0.8197 - precision_1: 0.8354\n",
      "Epoch 27/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4227 - recall_1: 0.8114 - accuracy: 0.8350 - precision_1: 0.8427\n",
      "Epoch 28/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4286 - recall_1: 0.7914 - accuracy: 0.8266 - precision_1: 0.8419\n",
      "Epoch 29/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4060 - recall_1: 0.8086 - accuracy: 0.8488 - precision_1: 0.8708\n",
      "Epoch 30/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3850 - recall_1: 0.8171 - accuracy: 0.8544 - precision_1: 0.8746\n",
      "Epoch 31/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3943 - recall_1: 0.8200 - accuracy: 0.8447 - precision_1: 0.8542\n",
      "Epoch 32/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3609 - recall_1: 0.8714 - accuracy: 0.8877 - precision_1: 0.8944\n",
      "Epoch 33/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3500 - recall_1: 0.8629 - accuracy: 0.8890 - precision_1: 0.9042\n",
      "Epoch 34/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3626 - recall_1: 0.8571 - accuracy: 0.8738 - precision_1: 0.8798\n",
      "Epoch 35/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3559 - recall_1: 0.8543 - accuracy: 0.8738 - precision_1: 0.8820\n",
      "Epoch 36/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3237 - recall_1: 0.8771 - accuracy: 0.8974 - precision_1: 0.9083\n",
      "Epoch 37/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3096 - recall_1: 0.8857 - accuracy: 0.9085 - precision_1: 0.9226\n",
      "Epoch 38/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3099 - recall_1: 0.8857 - accuracy: 0.9043 - precision_1: 0.9145\n",
      "Epoch 39/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2990 - recall_1: 0.9000 - accuracy: 0.9098 - precision_1: 0.9130\n",
      "Epoch 40/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2855 - recall_1: 0.8943 - accuracy: 0.9209 - precision_1: 0.9399\n",
      "Epoch 41/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2838 - recall_1: 0.9029 - accuracy: 0.9223 - precision_1: 0.9349\n",
      "Epoch 42/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2701 - recall_1: 0.9029 - accuracy: 0.9154 - precision_1: 0.9213\n",
      "Epoch 43/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2591 - recall_1: 0.9171 - accuracy: 0.9293 - precision_1: 0.9359\n",
      "Epoch 44/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2483 - recall_1: 0.9229 - accuracy: 0.9334 - precision_1: 0.9390\n",
      "Epoch 45/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2908 - recall_1: 0.9029 - accuracy: 0.9098 - precision_1: 0.9107\n",
      "Epoch 46/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2452 - recall_1: 0.9143 - accuracy: 0.9362 - precision_1: 0.9524\n",
      "Epoch 47/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2371 - recall_1: 0.9343 - accuracy: 0.9445 - precision_1: 0.9506\n",
      "Epoch 48/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2258 - recall_1: 0.9314 - accuracy: 0.9390 - precision_1: 0.9422\n",
      "Epoch 49/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2284 - recall_1: 0.9200 - accuracy: 0.9279 - precision_1: 0.9306\n",
      "Epoch 50/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2157 - recall_1: 0.9400 - accuracy: 0.9445 - precision_1: 0.9454\n",
      "Epoch 51/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2018 - recall_1: 0.9343 - accuracy: 0.9501 - precision_1: 0.9618\n",
      "Epoch 52/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2155 - recall_1: 0.9400 - accuracy: 0.9459 - precision_1: 0.9481\n",
      "Epoch 53/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2364 - recall_1: 0.9314 - accuracy: 0.9376 - precision_1: 0.9395\n",
      "Epoch 54/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2109 - recall_1: 0.9486 - accuracy: 0.9501 - precision_1: 0.9486\n",
      "Epoch 55/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1821 - recall_1: 0.9457 - accuracy: 0.9612 - precision_1: 0.9735\n",
      "Epoch 56/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1749 - recall_1: 0.9457 - accuracy: 0.9653 - precision_1: 0.9822\n",
      "Epoch 57/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1743 - recall_1: 0.9571 - accuracy: 0.9584 - precision_1: 0.9571\n",
      "Epoch 58/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1694 - recall_1: 0.9486 - accuracy: 0.9612 - precision_1: 0.9708\n",
      "Epoch 59/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1749 - recall_1: 0.9429 - accuracy: 0.9612 - precision_1: 0.9763\n",
      "Epoch 60/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1684 - recall_1: 0.9686 - accuracy: 0.9681 - precision_1: 0.9658\n",
      "Score for fold 2: loss of 1.3665492534637451; recall_1 of 61.764705181121826% ; accuracy of 55.55555820465088%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6354 - recall_2: 0.0634 - accuracy: 0.5208 - precision_2: 0.5116\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.8396 - recall_2: 0.4841 - accuracy: 0.5873 - precision_2: 0.5854\n",
      "Epoch 3/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6986 - recall_2: 0.4669 - accuracy: 0.5928 - precision_2: 0.5978\n",
      "Epoch 4/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6786 - recall_2: 0.5072 - accuracy: 0.6094 - precision_2: 0.6132\n",
      "Epoch 5/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6718 - recall_2: 0.5072 - accuracy: 0.5983 - precision_2: 0.5966\n",
      "Epoch 6/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6523 - recall_2: 0.5908 - accuracy: 0.6440 - precision_2: 0.6406\n",
      "Epoch 7/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6371 - recall_2: 0.6196 - accuracy: 0.6704 - precision_2: 0.6698\n",
      "Epoch 8/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6260 - recall_2: 0.6513 - accuracy: 0.6759 - precision_2: 0.6667\n",
      "Epoch 9/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6215 - recall_2: 0.6138 - accuracy: 0.6856 - precision_2: 0.6961\n",
      "Epoch 10/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5988 - recall_2: 0.6830 - accuracy: 0.7036 - precision_2: 0.6950\n",
      "Epoch 11/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5804 - recall_2: 0.6628 - accuracy: 0.7258 - precision_2: 0.7395\n",
      "Epoch 12/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5516 - recall_2: 0.7464 - accuracy: 0.7673 - precision_2: 0.7640\n",
      "Epoch 13/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5422 - recall_2: 0.7176 - accuracy: 0.7825 - precision_2: 0.8084\n",
      "Epoch 14/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5295 - recall_2: 0.7089 - accuracy: 0.7562 - precision_2: 0.7664\n",
      "Epoch 15/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5005 - recall_2: 0.7550 - accuracy: 0.7936 - precision_2: 0.8037\n",
      "Epoch 16/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4767 - recall_2: 0.7810 - accuracy: 0.8144 - precision_2: 0.8237\n",
      "Epoch 17/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4727 - recall_2: 0.7695 - accuracy: 0.8075 - precision_2: 0.8190\n",
      "Epoch 18/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4555 - recall_2: 0.7896 - accuracy: 0.8089 - precision_2: 0.8083\n",
      "Epoch 19/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4394 - recall_2: 0.7867 - accuracy: 0.8227 - precision_2: 0.8349\n",
      "Epoch 20/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4056 - recall_2: 0.8242 - accuracy: 0.8532 - precision_2: 0.8640\n",
      "Epoch 21/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3923 - recall_2: 0.8329 - accuracy: 0.8629 - precision_2: 0.8758\n",
      "Epoch 22/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3787 - recall_2: 0.8415 - accuracy: 0.8657 - precision_2: 0.8743\n",
      "Epoch 23/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3536 - recall_2: 0.8703 - accuracy: 0.8823 - precision_2: 0.8830\n",
      "Epoch 24/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3472 - recall_2: 0.8646 - accuracy: 0.8823 - precision_2: 0.8876\n",
      "Epoch 25/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3530 - recall_2: 0.8473 - accuracy: 0.8740 - precision_2: 0.8855\n",
      "Epoch 26/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3130 - recall_2: 0.8732 - accuracy: 0.9044 - precision_2: 0.9238\n",
      "Epoch 27/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3225 - recall_2: 0.8674 - accuracy: 0.8934 - precision_2: 0.9066\n",
      "Epoch 28/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2886 - recall_2: 0.9078 - accuracy: 0.9169 - precision_2: 0.9184\n",
      "Epoch 29/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3111 - recall_2: 0.8732 - accuracy: 0.8920 - precision_2: 0.8991\n",
      "Epoch 30/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2681 - recall_2: 0.9193 - accuracy: 0.9321 - precision_2: 0.9382\n",
      "Epoch 31/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2722 - recall_2: 0.9020 - accuracy: 0.9224 - precision_2: 0.9343\n",
      "Epoch 32/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2662 - recall_2: 0.9135 - accuracy: 0.9294 - precision_2: 0.9379\n",
      "Epoch 33/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2621 - recall_2: 0.9107 - accuracy: 0.9224 - precision_2: 0.9267\n",
      "Epoch 34/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2468 - recall_2: 0.9193 - accuracy: 0.9321 - precision_2: 0.9382\n",
      "Epoch 35/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2466 - recall_2: 0.9135 - accuracy: 0.9224 - precision_2: 0.9242\n",
      "Epoch 36/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2309 - recall_2: 0.9251 - accuracy: 0.9377 - precision_2: 0.9441\n",
      "Epoch 37/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2081 - recall_2: 0.9452 - accuracy: 0.9584 - precision_2: 0.9676\n",
      "Epoch 38/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2158 - recall_2: 0.9539 - accuracy: 0.9515 - precision_2: 0.9457\n",
      "Epoch 39/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2086 - recall_2: 0.9337 - accuracy: 0.9501 - precision_2: 0.9614\n",
      "Epoch 40/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2159 - recall_2: 0.9424 - accuracy: 0.9432 - precision_2: 0.9397\n",
      "Epoch 41/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1995 - recall_2: 0.9308 - accuracy: 0.9474 - precision_2: 0.9585\n",
      "Epoch 42/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1838 - recall_2: 0.9539 - accuracy: 0.9626 - precision_2: 0.9678\n",
      "Epoch 43/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1663 - recall_2: 0.9568 - accuracy: 0.9709 - precision_2: 0.9822\n",
      "Epoch 44/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1691 - recall_2: 0.9597 - accuracy: 0.9584 - precision_2: 0.9542\n",
      "Epoch 45/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1740 - recall_2: 0.9337 - accuracy: 0.9529 - precision_2: 0.9672\n",
      "Epoch 46/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1498 - recall_2: 0.9654 - accuracy: 0.9709 - precision_2: 0.9738\n",
      "Epoch 47/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1475 - recall_2: 0.9683 - accuracy: 0.9737 - precision_2: 0.9767\n",
      "Epoch 48/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1401 - recall_2: 0.9769 - accuracy: 0.9806 - precision_2: 0.9826\n",
      "Epoch 49/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1515 - recall_2: 0.9625 - accuracy: 0.9723 - precision_2: 0.9795\n",
      "Epoch 50/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1412 - recall_2: 0.9654 - accuracy: 0.9778 - precision_2: 0.9882\n",
      "Epoch 51/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1522 - recall_2: 0.9568 - accuracy: 0.9598 - precision_2: 0.9595\n",
      "Epoch 52/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1430 - recall_2: 0.9625 - accuracy: 0.9723 - precision_2: 0.9795\n",
      "Epoch 53/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1523 - recall_2: 0.9597 - accuracy: 0.9681 - precision_2: 0.9737\n",
      "Epoch 54/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1192 - recall_2: 0.9769 - accuracy: 0.9834 - precision_2: 0.9883\n",
      "Epoch 55/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1192 - recall_2: 0.9769 - accuracy: 0.9820 - precision_2: 0.9855\n",
      "Epoch 56/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1068 - recall_2: 0.9741 - accuracy: 0.9834 - precision_2: 0.9912\n",
      "Epoch 57/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0970 - recall_2: 0.9827 - accuracy: 0.9889 - precision_2: 0.9942\n",
      "Epoch 58/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0902 - recall_2: 0.9914 - accuracy: 0.9945 - precision_2: 0.9971\n",
      "Epoch 59/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0925 - recall_2: 0.9827 - accuracy: 0.9903 - precision_2: 0.9971\n",
      "Epoch 60/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0927 - recall_2: 0.9769 - accuracy: 0.9848 - precision_2: 0.9912\n",
      "Score for fold 3: loss of 1.4485677480697632; recall_2 of 62.162160873413086% ; accuracy of 62.5%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.6460 - recall_3: 0.0174 - accuracy: 0.5249 - precision_3: 0.5455\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.8429 - recall_3: 0.1977 - accuracy: 0.5319 - precision_3: 0.5231\n",
      "Epoch 3/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.7063 - recall_3: 0.2994 - accuracy: 0.5706 - precision_3: 0.5988\n",
      "Epoch 4/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6882 - recall_3: 0.2122 - accuracy: 0.5512 - precision_3: 0.5794\n",
      "Epoch 5/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.6758 - recall_3: 0.4593 - accuracy: 0.6233 - precision_3: 0.6475\n",
      "Epoch 6/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6605 - recall_3: 0.5262 - accuracy: 0.6371 - precision_3: 0.6464\n",
      "Epoch 7/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6454 - recall_3: 0.6017 - accuracy: 0.6634 - precision_3: 0.6613\n",
      "Epoch 8/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6291 - recall_3: 0.6192 - accuracy: 0.6994 - precision_3: 0.7124\n",
      "Epoch 9/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6109 - recall_3: 0.6424 - accuracy: 0.6939 - precision_3: 0.6928\n",
      "Epoch 10/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5923 - recall_3: 0.6686 - accuracy: 0.7175 - precision_3: 0.7188\n",
      "Epoch 11/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5741 - recall_3: 0.6744 - accuracy: 0.7188 - precision_3: 0.7183\n",
      "Epoch 12/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5474 - recall_3: 0.7267 - accuracy: 0.7618 - precision_3: 0.7622\n",
      "Epoch 13/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5308 - recall_3: 0.7238 - accuracy: 0.7618 - precision_3: 0.7638\n",
      "Epoch 14/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5106 - recall_3: 0.7529 - accuracy: 0.7881 - precision_3: 0.7920\n",
      "Epoch 15/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5012 - recall_3: 0.7616 - accuracy: 0.7895 - precision_3: 0.7892\n",
      "Epoch 16/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4922 - recall_3: 0.7413 - accuracy: 0.7742 - precision_3: 0.7751\n",
      "Epoch 17/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4726 - recall_3: 0.7645 - accuracy: 0.8144 - precision_3: 0.8323\n",
      "Epoch 18/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4387 - recall_3: 0.7907 - accuracy: 0.8283 - precision_3: 0.8395\n",
      "Epoch 19/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4392 - recall_3: 0.8198 - accuracy: 0.8283 - precision_3: 0.8198\n",
      "Epoch 20/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3963 - recall_3: 0.8314 - accuracy: 0.8601 - precision_3: 0.8693\n",
      "Epoch 21/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3993 - recall_3: 0.8110 - accuracy: 0.8463 - precision_3: 0.8585\n",
      "Epoch 22/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3807 - recall_3: 0.8401 - accuracy: 0.8629 - precision_3: 0.8679\n",
      "Epoch 23/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3567 - recall_3: 0.8547 - accuracy: 0.8809 - precision_3: 0.8909\n",
      "Epoch 24/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3375 - recall_3: 0.8692 - accuracy: 0.8934 - precision_3: 0.9033\n",
      "Epoch 25/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3244 - recall_3: 0.8953 - accuracy: 0.9100 - precision_3: 0.9139\n",
      "Epoch 26/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3263 - recall_3: 0.8808 - accuracy: 0.8864 - precision_3: 0.8808\n",
      "Epoch 27/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3027 - recall_3: 0.8721 - accuracy: 0.9030 - precision_3: 0.9202\n",
      "Epoch 28/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2882 - recall_3: 0.8953 - accuracy: 0.9141 - precision_3: 0.9222\n",
      "Epoch 29/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2856 - recall_3: 0.9041 - accuracy: 0.9211 - precision_3: 0.9284\n",
      "Epoch 30/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2710 - recall_3: 0.9012 - accuracy: 0.9238 - precision_3: 0.9366\n",
      "Epoch 31/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2620 - recall_3: 0.9012 - accuracy: 0.9294 - precision_3: 0.9480\n",
      "Epoch 32/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2441 - recall_3: 0.9244 - accuracy: 0.9404 - precision_3: 0.9493\n",
      "Epoch 33/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2531 - recall_3: 0.9215 - accuracy: 0.9349 - precision_3: 0.9407\n",
      "Epoch 34/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2321 - recall_3: 0.9157 - accuracy: 0.9321 - precision_3: 0.9403\n",
      "Epoch 35/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2246 - recall_3: 0.9186 - accuracy: 0.9349 - precision_3: 0.9433\n",
      "Epoch 36/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2084 - recall_3: 0.9302 - accuracy: 0.9460 - precision_3: 0.9552\n",
      "Epoch 37/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2262 - recall_3: 0.9302 - accuracy: 0.9377 - precision_3: 0.9384\n",
      "Epoch 38/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1934 - recall_3: 0.9506 - accuracy: 0.9557 - precision_3: 0.9561\n",
      "Epoch 39/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1775 - recall_3: 0.9535 - accuracy: 0.9612 - precision_3: 0.9647\n",
      "Epoch 40/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1732 - recall_3: 0.9448 - accuracy: 0.9584 - precision_3: 0.9673\n",
      "Epoch 41/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1818 - recall_3: 0.9419 - accuracy: 0.9501 - precision_3: 0.9529\n",
      "Epoch 42/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1809 - recall_3: 0.9390 - accuracy: 0.9501 - precision_3: 0.9556\n",
      "Epoch 43/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1778 - recall_3: 0.9506 - accuracy: 0.9557 - precision_3: 0.9561\n",
      "Epoch 44/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1965 - recall_3: 0.9360 - accuracy: 0.9460 - precision_3: 0.9499\n",
      "Epoch 45/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1686 - recall_3: 0.9535 - accuracy: 0.9529 - precision_3: 0.9480\n",
      "Epoch 46/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1541 - recall_3: 0.9593 - accuracy: 0.9668 - precision_3: 0.9706\n",
      "Epoch 47/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1578 - recall_3: 0.9651 - accuracy: 0.9654 - precision_3: 0.9623\n",
      "Epoch 48/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1495 - recall_3: 0.9564 - accuracy: 0.9640 - precision_3: 0.9676\n",
      "Epoch 49/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1298 - recall_3: 0.9651 - accuracy: 0.9737 - precision_3: 0.9794\n",
      "Epoch 50/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1279 - recall_3: 0.9826 - accuracy: 0.9820 - precision_3: 0.9797\n",
      "Epoch 51/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1218 - recall_3: 0.9767 - accuracy: 0.9792 - precision_3: 0.9796\n",
      "Epoch 52/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1189 - recall_3: 0.9767 - accuracy: 0.9792 - precision_3: 0.9796\n",
      "Epoch 53/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1106 - recall_3: 0.9738 - accuracy: 0.9806 - precision_3: 0.9853\n",
      "Epoch 54/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1105 - recall_3: 0.9767 - accuracy: 0.9848 - precision_3: 0.9912\n",
      "Epoch 55/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1213 - recall_3: 0.9651 - accuracy: 0.9737 - precision_3: 0.9794\n",
      "Epoch 56/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1073 - recall_3: 0.9797 - accuracy: 0.9820 - precision_3: 0.9825\n",
      "Epoch 57/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1021 - recall_3: 0.9797 - accuracy: 0.9820 - precision_3: 0.9825\n",
      "Epoch 58/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0864 - recall_3: 0.9942 - accuracy: 0.9945 - precision_3: 0.9942\n",
      "Epoch 59/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0918 - recall_3: 0.9884 - accuracy: 0.9903 - precision_3: 0.9913\n",
      "Epoch 60/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1003 - recall_3: 0.9797 - accuracy: 0.9834 - precision_3: 0.9854\n",
      "Score for fold 4: loss of 1.8024698495864868; recall_3 of 44.999998807907104% ; accuracy of 60.00000238418579%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 1.6248 - recall_4: 0.3401 - accuracy: 0.5125 - precision_4: 0.4896\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.8398 - recall_4: 0.4294 - accuracy: 0.5291 - precision_4: 0.5120\n",
      "Epoch 3/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.7050 - recall_4: 0.2622 - accuracy: 0.5803 - precision_4: 0.6594\n",
      "Epoch 4/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6897 - recall_4: 0.4294 - accuracy: 0.5582 - precision_4: 0.5519\n",
      "Epoch 5/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6899 - recall_4: 0.3141 - accuracy: 0.5693 - precision_4: 0.5989\n",
      "Epoch 6/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6675 - recall_4: 0.4582 - accuracy: 0.6288 - precision_4: 0.6653\n",
      "Epoch 7/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6677 - recall_4: 0.5562 - accuracy: 0.6039 - precision_4: 0.5938\n",
      "Epoch 8/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6467 - recall_4: 0.5965 - accuracy: 0.6537 - precision_4: 0.6530\n",
      "Epoch 9/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6342 - recall_4: 0.5706 - accuracy: 0.6482 - precision_4: 0.6535\n",
      "Epoch 10/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6120 - recall_4: 0.6744 - accuracy: 0.7188 - precision_4: 0.7222\n",
      "Epoch 11/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5978 - recall_4: 0.6282 - accuracy: 0.7050 - precision_4: 0.7219\n",
      "Epoch 12/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5961 - recall_4: 0.6196 - accuracy: 0.7091 - precision_4: 0.7338\n",
      "Epoch 13/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5671 - recall_4: 0.6772 - accuracy: 0.7368 - precision_4: 0.7508\n",
      "Epoch 14/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5484 - recall_4: 0.7291 - accuracy: 0.7507 - precision_4: 0.7463\n",
      "Epoch 15/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5297 - recall_4: 0.7233 - accuracy: 0.7632 - precision_4: 0.7699\n",
      "Epoch 16/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5094 - recall_4: 0.7262 - accuracy: 0.7798 - precision_4: 0.7975\n",
      "Epoch 17/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5009 - recall_4: 0.7205 - accuracy: 0.7729 - precision_4: 0.7886\n",
      "Epoch 18/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4893 - recall_4: 0.7378 - accuracy: 0.7770 - precision_4: 0.7853\n",
      "Epoch 19/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4686 - recall_4: 0.7320 - accuracy: 0.8033 - precision_4: 0.8383\n",
      "Epoch 20/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4574 - recall_4: 0.7896 - accuracy: 0.8075 - precision_4: 0.8059\n",
      "Epoch 21/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4282 - recall_4: 0.8012 - accuracy: 0.8393 - precision_4: 0.8554\n",
      "Epoch 22/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4508 - recall_4: 0.7781 - accuracy: 0.8186 - precision_4: 0.8333\n",
      "Epoch 23/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4170 - recall_4: 0.7925 - accuracy: 0.8463 - precision_4: 0.8758\n",
      "Epoch 24/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4054 - recall_4: 0.7867 - accuracy: 0.8352 - precision_4: 0.8585\n",
      "Epoch 25/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3822 - recall_4: 0.8329 - accuracy: 0.8657 - precision_4: 0.8811\n",
      "Epoch 26/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3712 - recall_4: 0.8156 - accuracy: 0.8629 - precision_4: 0.8899\n",
      "Epoch 27/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3652 - recall_4: 0.8300 - accuracy: 0.8615 - precision_4: 0.8754\n",
      "Epoch 28/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3402 - recall_4: 0.8588 - accuracy: 0.8920 - precision_4: 0.9113\n",
      "Epoch 29/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3166 - recall_4: 0.8501 - accuracy: 0.8947 - precision_4: 0.9248\n",
      "Epoch 30/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3154 - recall_4: 0.8617 - accuracy: 0.8961 - precision_4: 0.9172\n",
      "Epoch 31/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3260 - recall_4: 0.8646 - accuracy: 0.8906 - precision_4: 0.9036\n",
      "Epoch 32/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2798 - recall_4: 0.8818 - accuracy: 0.9155 - precision_4: 0.9387\n",
      "Epoch 33/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2888 - recall_4: 0.8876 - accuracy: 0.9100 - precision_4: 0.9222\n",
      "Epoch 34/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2776 - recall_4: 0.8963 - accuracy: 0.9114 - precision_4: 0.9174\n",
      "Epoch 35/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2681 - recall_4: 0.8963 - accuracy: 0.9044 - precision_4: 0.9041\n",
      "Epoch 36/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2535 - recall_4: 0.9049 - accuracy: 0.9335 - precision_4: 0.9544\n",
      "Epoch 37/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2367 - recall_4: 0.8991 - accuracy: 0.9266 - precision_4: 0.9455\n",
      "Epoch 38/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2330 - recall_4: 0.9107 - accuracy: 0.9321 - precision_4: 0.9461\n",
      "Epoch 39/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2301 - recall_4: 0.9135 - accuracy: 0.9349 - precision_4: 0.9491\n",
      "Epoch 40/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2104 - recall_4: 0.9164 - accuracy: 0.9418 - precision_4: 0.9607\n",
      "Epoch 41/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2002 - recall_4: 0.9337 - accuracy: 0.9488 - precision_4: 0.9586\n",
      "Epoch 42/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2001 - recall_4: 0.9193 - accuracy: 0.9432 - precision_4: 0.9608\n",
      "Epoch 43/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2017 - recall_4: 0.9135 - accuracy: 0.9418 - precision_4: 0.9635\n",
      "Epoch 44/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1903 - recall_4: 0.9251 - accuracy: 0.9529 - precision_4: 0.9757\n",
      "Epoch 45/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2181 - recall_4: 0.9135 - accuracy: 0.9404 - precision_4: 0.9606\n",
      "Epoch 46/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1715 - recall_4: 0.9452 - accuracy: 0.9598 - precision_4: 0.9704\n",
      "Epoch 47/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1728 - recall_4: 0.9597 - accuracy: 0.9640 - precision_4: 0.9652\n",
      "Epoch 48/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1506 - recall_4: 0.9539 - accuracy: 0.9681 - precision_4: 0.9793\n",
      "Epoch 49/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1549 - recall_4: 0.9625 - accuracy: 0.9709 - precision_4: 0.9766\n",
      "Epoch 50/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1502 - recall_4: 0.9597 - accuracy: 0.9737 - precision_4: 0.9852\n",
      "Epoch 51/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1614 - recall_4: 0.9481 - accuracy: 0.9557 - precision_4: 0.9592\n",
      "Epoch 52/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1473 - recall_4: 0.9539 - accuracy: 0.9681 - precision_4: 0.9793\n",
      "Epoch 53/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1349 - recall_4: 0.9683 - accuracy: 0.9778 - precision_4: 0.9853\n",
      "Epoch 54/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1353 - recall_4: 0.9597 - accuracy: 0.9723 - precision_4: 0.9823\n",
      "Epoch 55/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1377 - recall_4: 0.9625 - accuracy: 0.9695 - precision_4: 0.9738\n",
      "Epoch 56/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1299 - recall_4: 0.9769 - accuracy: 0.9806 - precision_4: 0.9826\n",
      "Epoch 57/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1274 - recall_4: 0.9712 - accuracy: 0.9792 - precision_4: 0.9854\n",
      "Epoch 58/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1208 - recall_4: 0.9769 - accuracy: 0.9834 - precision_4: 0.9883\n",
      "Epoch 59/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1100 - recall_4: 0.9798 - accuracy: 0.9848 - precision_4: 0.9884\n",
      "Epoch 60/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1162 - recall_4: 0.9741 - accuracy: 0.9778 - precision_4: 0.9797\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fc4abbb7510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Score for fold 5: loss of 1.949022650718689; recall_4 of 70.27027010917664% ; accuracy of 64.99999761581421%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.6153 - recall_5: 0.1853 - accuracy: 0.5235 - precision_5: 0.4846\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.8375 - recall_5: 0.0118 - accuracy: 0.5319 - precision_5: 0.6667\n",
      "Epoch 3/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.7076 - recall_5: 0.0147 - accuracy: 0.5332 - precision_5: 0.7143\n",
      "Epoch 4/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6921 - recall_5: 0.0118 - accuracy: 0.5291 - precision_5: 0.5000\n",
      "Epoch 5/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6915 - recall_5: 0.3029 - accuracy: 0.5346 - precision_5: 0.5099\n",
      "Epoch 6/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6844 - recall_5: 0.2971 - accuracy: 0.5997 - precision_5: 0.6689\n",
      "Epoch 7/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6847 - recall_5: 0.4647 - accuracy: 0.5914 - precision_5: 0.5830\n",
      "Epoch 8/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6699 - recall_5: 0.4500 - accuracy: 0.6136 - precision_5: 0.6245\n",
      "Epoch 9/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6669 - recall_5: 0.5618 - accuracy: 0.6316 - precision_5: 0.6201\n",
      "Epoch 10/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6534 - recall_5: 0.5412 - accuracy: 0.6496 - precision_5: 0.6548\n",
      "Epoch 11/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6367 - recall_5: 0.6471 - accuracy: 0.6731 - precision_5: 0.6548\n",
      "Epoch 12/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6223 - recall_5: 0.6029 - accuracy: 0.6884 - precision_5: 0.6949\n",
      "Epoch 13/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6047 - recall_5: 0.6588 - accuracy: 0.7133 - precision_5: 0.7111\n",
      "Epoch 14/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5869 - recall_5: 0.6529 - accuracy: 0.7133 - precision_5: 0.7138\n",
      "Epoch 15/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5665 - recall_5: 0.6618 - accuracy: 0.7368 - precision_5: 0.7500\n",
      "Epoch 16/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5574 - recall_5: 0.6765 - accuracy: 0.7562 - precision_5: 0.7770\n",
      "Epoch 17/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5425 - recall_5: 0.6882 - accuracy: 0.7562 - precision_5: 0.7697\n",
      "Epoch 18/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5200 - recall_5: 0.7529 - accuracy: 0.7881 - precision_5: 0.7877\n",
      "Epoch 19/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5111 - recall_5: 0.7265 - accuracy: 0.7729 - precision_5: 0.7767\n",
      "Epoch 20/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4913 - recall_5: 0.7382 - accuracy: 0.7895 - precision_5: 0.7994\n",
      "Epoch 21/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4793 - recall_5: 0.7529 - accuracy: 0.8019 - precision_5: 0.8127\n",
      "Epoch 22/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4729 - recall_5: 0.7882 - accuracy: 0.7978 - precision_5: 0.7836\n",
      "Epoch 23/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4622 - recall_5: 0.7559 - accuracy: 0.8006 - precision_5: 0.8082\n",
      "Epoch 24/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4659 - recall_5: 0.7471 - accuracy: 0.8006 - precision_5: 0.8141\n",
      "Epoch 25/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4201 - recall_5: 0.8088 - accuracy: 0.8435 - precision_5: 0.8514\n",
      "Epoch 26/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4159 - recall_5: 0.8265 - accuracy: 0.8490 - precision_5: 0.8489\n",
      "Epoch 27/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3973 - recall_5: 0.8412 - accuracy: 0.8670 - precision_5: 0.8720\n",
      "Epoch 28/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3862 - recall_5: 0.8176 - accuracy: 0.8532 - precision_5: 0.8634\n",
      "Epoch 29/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3857 - recall_5: 0.8147 - accuracy: 0.8546 - precision_5: 0.8683\n",
      "Epoch 30/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3588 - recall_5: 0.8500 - accuracy: 0.8878 - precision_5: 0.9060\n",
      "Epoch 31/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3503 - recall_5: 0.8559 - accuracy: 0.8823 - precision_5: 0.8899\n",
      "Epoch 32/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3423 - recall_5: 0.8735 - accuracy: 0.8906 - precision_5: 0.8919\n",
      "Epoch 33/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3310 - recall_5: 0.8676 - accuracy: 0.8934 - precision_5: 0.9021\n",
      "Epoch 34/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3079 - recall_5: 0.8765 - accuracy: 0.9058 - precision_5: 0.9198\n",
      "Epoch 35/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3044 - recall_5: 0.8735 - accuracy: 0.9058 - precision_5: 0.9224\n",
      "Epoch 36/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3008 - recall_5: 0.9000 - accuracy: 0.9086 - precision_5: 0.9053\n",
      "Epoch 37/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2872 - recall_5: 0.8882 - accuracy: 0.9086 - precision_5: 0.9152\n",
      "Epoch 38/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3002 - recall_5: 0.8735 - accuracy: 0.8934 - precision_5: 0.8973\n",
      "Epoch 39/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2610 - recall_5: 0.9029 - accuracy: 0.9321 - precision_5: 0.9505\n",
      "Epoch 40/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2653 - recall_5: 0.9088 - accuracy: 0.9335 - precision_5: 0.9479\n",
      "Epoch 41/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2571 - recall_5: 0.9176 - accuracy: 0.9321 - precision_5: 0.9369\n",
      "Epoch 42/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2418 - recall_5: 0.9235 - accuracy: 0.9418 - precision_5: 0.9515\n",
      "Epoch 43/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2252 - recall_5: 0.9265 - accuracy: 0.9501 - precision_5: 0.9663\n",
      "Epoch 44/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2296 - recall_5: 0.9088 - accuracy: 0.9432 - precision_5: 0.9687\n",
      "Epoch 45/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2071 - recall_5: 0.9206 - accuracy: 0.9474 - precision_5: 0.9660\n",
      "Epoch 46/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2089 - recall_5: 0.9353 - accuracy: 0.9529 - precision_5: 0.9636\n",
      "Epoch 47/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1913 - recall_5: 0.9412 - accuracy: 0.9584 - precision_5: 0.9697\n",
      "Epoch 48/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1943 - recall_5: 0.9559 - accuracy: 0.9654 - precision_5: 0.9701\n",
      "Epoch 49/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1903 - recall_5: 0.9294 - accuracy: 0.9529 - precision_5: 0.9693\n",
      "Epoch 50/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1862 - recall_5: 0.9412 - accuracy: 0.9543 - precision_5: 0.9610\n",
      "Epoch 51/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1826 - recall_5: 0.9441 - accuracy: 0.9668 - precision_5: 0.9847\n",
      "Epoch 52/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1769 - recall_5: 0.9500 - accuracy: 0.9654 - precision_5: 0.9758\n",
      "Epoch 53/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1624 - recall_5: 0.9559 - accuracy: 0.9709 - precision_5: 0.9819\n",
      "Epoch 54/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1563 - recall_5: 0.9559 - accuracy: 0.9695 - precision_5: 0.9789\n",
      "Epoch 55/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1547 - recall_5: 0.9676 - accuracy: 0.9709 - precision_5: 0.9705\n",
      "Epoch 56/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1566 - recall_5: 0.9471 - accuracy: 0.9668 - precision_5: 0.9817\n",
      "Epoch 57/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1519 - recall_5: 0.9647 - accuracy: 0.9737 - precision_5: 0.9791\n",
      "Epoch 58/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1439 - recall_5: 0.9618 - accuracy: 0.9737 - precision_5: 0.9820\n",
      "Epoch 59/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1431 - recall_5: 0.9529 - accuracy: 0.9723 - precision_5: 0.9878\n",
      "Epoch 60/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1307 - recall_5: 0.9706 - accuracy: 0.9806 - precision_5: 0.9880\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fc4ab0d4ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Score for fold 6: loss of 1.3497289419174194; recall_5 of 68.18181872367859% ; accuracy of 60.00000238418579%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.6470 - recall_6: 0.0175 - accuracy: 0.5152 - precision_6: 0.3000\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.8446 - recall_6: 0.0263 - accuracy: 0.5360 - precision_6: 0.8182\n",
      "Epoch 3/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.7084 - recall_6: 0.0000e+00 - accuracy: 0.5263 - precision_6: 0.0000e+00\n",
      "Epoch 4/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6893 - recall_6: 0.0760 - accuracy: 0.5499 - precision_6: 0.7429\n",
      "Epoch 5/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6810 - recall_6: 0.4298 - accuracy: 0.6163 - precision_6: 0.6419\n",
      "Epoch 6/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6756 - recall_6: 0.3801 - accuracy: 0.5997 - precision_6: 0.6280\n",
      "Epoch 7/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6654 - recall_6: 0.5205 - accuracy: 0.6136 - precision_6: 0.6075\n",
      "Epoch 8/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6558 - recall_6: 0.5702 - accuracy: 0.6413 - precision_6: 0.6352\n",
      "Epoch 9/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6422 - recall_6: 0.5205 - accuracy: 0.6593 - precision_6: 0.6846\n",
      "Epoch 10/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6266 - recall_6: 0.6608 - accuracy: 0.6870 - precision_6: 0.6726\n",
      "Epoch 11/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6172 - recall_6: 0.6404 - accuracy: 0.6870 - precision_6: 0.6801\n",
      "Epoch 12/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5964 - recall_6: 0.6784 - accuracy: 0.7161 - precision_6: 0.7095\n",
      "Epoch 13/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5851 - recall_6: 0.6725 - accuracy: 0.7188 - precision_6: 0.7165\n",
      "Epoch 14/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5817 - recall_6: 0.6901 - accuracy: 0.7202 - precision_6: 0.7108\n",
      "Epoch 15/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5604 - recall_6: 0.6725 - accuracy: 0.7382 - precision_6: 0.7492\n",
      "Epoch 16/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5539 - recall_6: 0.7281 - accuracy: 0.7507 - precision_6: 0.7411\n",
      "Epoch 17/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5323 - recall_6: 0.7164 - accuracy: 0.7618 - precision_6: 0.7656\n",
      "Epoch 18/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5120 - recall_6: 0.7485 - accuracy: 0.7825 - precision_6: 0.7829\n",
      "Epoch 19/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4991 - recall_6: 0.7339 - accuracy: 0.7798 - precision_6: 0.7868\n",
      "Epoch 20/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4873 - recall_6: 0.7544 - accuracy: 0.7895 - precision_6: 0.7914\n",
      "Epoch 21/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4841 - recall_6: 0.7953 - accuracy: 0.8047 - precision_6: 0.7930\n",
      "Epoch 22/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4558 - recall_6: 0.8158 - accuracy: 0.8241 - precision_6: 0.8134\n",
      "Epoch 23/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4455 - recall_6: 0.7953 - accuracy: 0.8255 - precision_6: 0.8293\n",
      "Epoch 24/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4352 - recall_6: 0.8070 - accuracy: 0.8241 - precision_6: 0.8190\n",
      "Epoch 25/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4169 - recall_6: 0.8304 - accuracy: 0.8560 - precision_6: 0.8606\n",
      "Epoch 26/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4018 - recall_6: 0.8333 - accuracy: 0.8393 - precision_6: 0.8285\n",
      "Epoch 27/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3810 - recall_6: 0.8450 - accuracy: 0.8670 - precision_6: 0.8705\n",
      "Epoch 28/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3788 - recall_6: 0.8392 - accuracy: 0.8670 - precision_6: 0.8750\n",
      "Epoch 29/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3537 - recall_6: 0.8772 - accuracy: 0.8795 - precision_6: 0.8696\n",
      "Epoch 30/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3525 - recall_6: 0.8450 - accuracy: 0.8726 - precision_6: 0.8811\n",
      "Epoch 31/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3241 - recall_6: 0.8889 - accuracy: 0.9003 - precision_6: 0.8994\n",
      "Epoch 32/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3510 - recall_6: 0.8450 - accuracy: 0.8684 - precision_6: 0.8731\n",
      "Epoch 33/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3075 - recall_6: 0.8830 - accuracy: 0.9030 - precision_6: 0.9096\n",
      "Epoch 34/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3280 - recall_6: 0.8655 - accuracy: 0.8892 - precision_6: 0.8970\n",
      "Epoch 35/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2935 - recall_6: 0.8947 - accuracy: 0.9017 - precision_6: 0.8974\n",
      "Epoch 36/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2872 - recall_6: 0.9006 - accuracy: 0.9211 - precision_6: 0.9305\n",
      "Epoch 37/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2659 - recall_6: 0.9152 - accuracy: 0.9224 - precision_6: 0.9206\n",
      "Epoch 38/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2683 - recall_6: 0.9035 - accuracy: 0.9155 - precision_6: 0.9169\n",
      "Epoch 39/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2400 - recall_6: 0.9123 - accuracy: 0.9321 - precision_6: 0.9426\n",
      "Epoch 40/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2324 - recall_6: 0.9211 - accuracy: 0.9363 - precision_6: 0.9431\n",
      "Epoch 41/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2292 - recall_6: 0.9298 - accuracy: 0.9391 - precision_6: 0.9408\n",
      "Epoch 42/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2337 - recall_6: 0.9240 - accuracy: 0.9391 - precision_6: 0.9461\n",
      "Epoch 43/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2331 - recall_6: 0.9240 - accuracy: 0.9321 - precision_6: 0.9322\n",
      "Epoch 44/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2164 - recall_6: 0.9386 - accuracy: 0.9501 - precision_6: 0.9554\n",
      "Epoch 45/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2147 - recall_6: 0.9415 - accuracy: 0.9391 - precision_6: 0.9306\n",
      "Epoch 46/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1969 - recall_6: 0.9561 - accuracy: 0.9571 - precision_6: 0.9534\n",
      "Epoch 47/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1837 - recall_6: 0.9503 - accuracy: 0.9571 - precision_6: 0.9587\n",
      "Epoch 48/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1828 - recall_6: 0.9561 - accuracy: 0.9598 - precision_6: 0.9589\n",
      "Epoch 49/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1877 - recall_6: 0.9503 - accuracy: 0.9598 - precision_6: 0.9644\n",
      "Epoch 50/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1750 - recall_6: 0.9444 - accuracy: 0.9654 - precision_6: 0.9818\n",
      "Epoch 51/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1615 - recall_6: 0.9591 - accuracy: 0.9695 - precision_6: 0.9762\n",
      "Epoch 52/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1597 - recall_6: 0.9708 - accuracy: 0.9681 - precision_6: 0.9623\n",
      "Epoch 53/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1518 - recall_6: 0.9678 - accuracy: 0.9723 - precision_6: 0.9735\n",
      "Epoch 54/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1793 - recall_6: 0.9474 - accuracy: 0.9543 - precision_6: 0.9558\n",
      "Epoch 55/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1434 - recall_6: 0.9708 - accuracy: 0.9778 - precision_6: 0.9822\n",
      "Epoch 56/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2531 - recall_6: 0.9035 - accuracy: 0.9169 - precision_6: 0.9196\n",
      "Epoch 57/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1510 - recall_6: 0.9678 - accuracy: 0.9751 - precision_6: 0.9793\n",
      "Epoch 58/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1366 - recall_6: 0.9649 - accuracy: 0.9737 - precision_6: 0.9792\n",
      "Epoch 59/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1354 - recall_6: 0.9708 - accuracy: 0.9806 - precision_6: 0.9881\n",
      "Epoch 60/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1262 - recall_6: 0.9737 - accuracy: 0.9820 - precision_6: 0.9881\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fc4a9534620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Score for fold 7: loss of 1.0444746017456055; recall_6 of 66.66666865348816% ; accuracy of 68.75%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.6324 - recall_7: 0.7168 - accuracy: 0.4737 - precision_7: 0.4679\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.8416 - recall_7: 0.2023 - accuracy: 0.5568 - precision_7: 0.6140\n",
      "Epoch 3/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.7065 - recall_7: 0.3468 - accuracy: 0.6066 - precision_7: 0.6742\n",
      "Epoch 4/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6888 - recall_7: 0.6272 - accuracy: 0.5900 - precision_7: 0.5651\n",
      "Epoch 5/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6827 - recall_7: 0.5434 - accuracy: 0.5983 - precision_7: 0.5875\n",
      "Epoch 6/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6662 - recall_7: 0.6676 - accuracy: 0.6385 - precision_7: 0.6127\n",
      "Epoch 7/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6614 - recall_7: 0.4855 - accuracy: 0.6260 - precision_7: 0.6462\n",
      "Epoch 8/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6615 - recall_7: 0.5751 - accuracy: 0.6177 - precision_7: 0.6067\n",
      "Epoch 9/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6300 - recall_7: 0.6127 - accuracy: 0.6759 - precision_7: 0.6795\n",
      "Epoch 10/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6281 - recall_7: 0.5867 - accuracy: 0.6773 - precision_7: 0.6928\n",
      "Epoch 11/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6067 - recall_7: 0.6590 - accuracy: 0.6981 - precision_7: 0.6951\n",
      "Epoch 12/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5867 - recall_7: 0.6734 - accuracy: 0.7258 - precision_7: 0.7327\n",
      "Epoch 13/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.5780 - recall_7: 0.6590 - accuracy: 0.7188 - precision_7: 0.7284\n",
      "Epoch 14/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5604 - recall_7: 0.6879 - accuracy: 0.7424 - precision_7: 0.7532\n",
      "Epoch 15/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5368 - recall_7: 0.6994 - accuracy: 0.7784 - precision_7: 0.8121\n",
      "Epoch 16/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5486 - recall_7: 0.7283 - accuracy: 0.7673 - precision_7: 0.7730\n",
      "Epoch 17/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5135 - recall_7: 0.7370 - accuracy: 0.7812 - precision_7: 0.7919\n",
      "Epoch 18/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4959 - recall_7: 0.7428 - accuracy: 0.7909 - precision_7: 0.8056\n",
      "Epoch 19/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4795 - recall_7: 0.7775 - accuracy: 0.8144 - precision_7: 0.8252\n",
      "Epoch 20/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4608 - recall_7: 0.7861 - accuracy: 0.8227 - precision_7: 0.8344\n",
      "Epoch 21/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4509 - recall_7: 0.8006 - accuracy: 0.8269 - precision_7: 0.8318\n",
      "Epoch 22/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4281 - recall_7: 0.7890 - accuracy: 0.8407 - precision_7: 0.8667\n",
      "Epoch 23/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4198 - recall_7: 0.8295 - accuracy: 0.8490 - precision_7: 0.8516\n",
      "Epoch 24/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4394 - recall_7: 0.7890 - accuracy: 0.8324 - precision_7: 0.8505\n",
      "Epoch 25/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4067 - recall_7: 0.8382 - accuracy: 0.8504 - precision_7: 0.8480\n",
      "Epoch 26/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3841 - recall_7: 0.8410 - accuracy: 0.8587 - precision_7: 0.8609\n",
      "Epoch 27/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3812 - recall_7: 0.8382 - accuracy: 0.8601 - precision_7: 0.8657\n",
      "Epoch 28/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3896 - recall_7: 0.8266 - accuracy: 0.8504 - precision_7: 0.8563\n",
      "Epoch 29/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3616 - recall_7: 0.8671 - accuracy: 0.8767 - precision_7: 0.8746\n",
      "Epoch 30/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3470 - recall_7: 0.8815 - accuracy: 0.9003 - precision_7: 0.9077\n",
      "Epoch 31/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3314 - recall_7: 0.8584 - accuracy: 0.8892 - precision_7: 0.9055\n",
      "Epoch 32/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3267 - recall_7: 0.8642 - accuracy: 0.8864 - precision_7: 0.8952\n",
      "Epoch 33/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3132 - recall_7: 0.8642 - accuracy: 0.9003 - precision_7: 0.9228\n",
      "Epoch 34/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3010 - recall_7: 0.8960 - accuracy: 0.9169 - precision_7: 0.9281\n",
      "Epoch 35/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2952 - recall_7: 0.8728 - accuracy: 0.9003 - precision_7: 0.9152\n",
      "Epoch 36/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2843 - recall_7: 0.8699 - accuracy: 0.9072 - precision_7: 0.9319\n",
      "Epoch 37/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2926 - recall_7: 0.9104 - accuracy: 0.9114 - precision_7: 0.9052\n",
      "Epoch 38/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2776 - recall_7: 0.9017 - accuracy: 0.9141 - precision_7: 0.9176\n",
      "Epoch 39/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2717 - recall_7: 0.8902 - accuracy: 0.9127 - precision_7: 0.9249\n",
      "Epoch 40/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2604 - recall_7: 0.9075 - accuracy: 0.9294 - precision_7: 0.9429\n",
      "Epoch 41/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2434 - recall_7: 0.9104 - accuracy: 0.9377 - precision_7: 0.9574\n",
      "Epoch 42/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2532 - recall_7: 0.9162 - accuracy: 0.9280 - precision_7: 0.9324\n",
      "Epoch 43/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2294 - recall_7: 0.9162 - accuracy: 0.9404 - precision_7: 0.9577\n",
      "Epoch 44/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2135 - recall_7: 0.9220 - accuracy: 0.9404 - precision_7: 0.9522\n",
      "Epoch 45/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2088 - recall_7: 0.9335 - accuracy: 0.9446 - precision_7: 0.9500\n",
      "Epoch 46/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2127 - recall_7: 0.9480 - accuracy: 0.9474 - precision_7: 0.9425\n",
      "Epoch 47/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2145 - recall_7: 0.9249 - accuracy: 0.9446 - precision_7: 0.9581\n",
      "Epoch 48/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1900 - recall_7: 0.9480 - accuracy: 0.9598 - precision_7: 0.9676\n",
      "Epoch 49/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1928 - recall_7: 0.9480 - accuracy: 0.9529 - precision_7: 0.9535\n",
      "Epoch 50/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1818 - recall_7: 0.9480 - accuracy: 0.9543 - precision_7: 0.9563\n",
      "Epoch 51/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1920 - recall_7: 0.9422 - accuracy: 0.9488 - precision_7: 0.9504\n",
      "Epoch 52/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1993 - recall_7: 0.9306 - accuracy: 0.9446 - precision_7: 0.9527\n",
      "Epoch 53/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1604 - recall_7: 0.9566 - accuracy: 0.9681 - precision_7: 0.9764\n",
      "Epoch 54/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1629 - recall_7: 0.9538 - accuracy: 0.9612 - precision_7: 0.9649\n",
      "Epoch 55/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1560 - recall_7: 0.9595 - accuracy: 0.9654 - precision_7: 0.9679\n",
      "Epoch 56/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1497 - recall_7: 0.9711 - accuracy: 0.9778 - precision_7: 0.9825\n",
      "Epoch 57/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1485 - recall_7: 0.9624 - accuracy: 0.9695 - precision_7: 0.9737\n",
      "Epoch 58/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1378 - recall_7: 0.9682 - accuracy: 0.9765 - precision_7: 0.9824\n",
      "Epoch 59/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1403 - recall_7: 0.9740 - accuracy: 0.9778 - precision_7: 0.9797\n",
      "Epoch 60/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1316 - recall_7: 0.9740 - accuracy: 0.9765 - precision_7: 0.9768\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fc4aa7c2620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Score for fold 8: loss of 1.268777847290039; recall_7 of 55.263155698776245% ; accuracy of 63.749998807907104%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.5947 - recall_8: 0.0771 - accuracy: 0.5111 - precision_8: 0.4737\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.8328 - recall_8: 0.1343 - accuracy: 0.5416 - precision_8: 0.6267\n",
      "Epoch 3/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.7071 - recall_8: 0.4057 - accuracy: 0.5706 - precision_8: 0.5820\n",
      "Epoch 4/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6878 - recall_8: 0.4800 - accuracy: 0.5970 - precision_8: 0.6065\n",
      "Epoch 5/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6804 - recall_8: 0.4171 - accuracy: 0.5873 - precision_8: 0.6083\n",
      "Epoch 6/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6690 - recall_8: 0.5343 - accuracy: 0.6330 - precision_8: 0.6471\n",
      "Epoch 7/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6624 - recall_8: 0.6143 - accuracy: 0.6274 - precision_8: 0.6160\n",
      "Epoch 8/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6423 - recall_8: 0.6400 - accuracy: 0.6579 - precision_8: 0.6493\n",
      "Epoch 9/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6347 - recall_8: 0.6486 - accuracy: 0.6759 - precision_8: 0.6716\n",
      "Epoch 10/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6168 - recall_8: 0.6686 - accuracy: 0.6884 - precision_8: 0.6822\n",
      "Epoch 11/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6131 - recall_8: 0.7000 - accuracy: 0.6981 - precision_8: 0.6844\n",
      "Epoch 12/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5904 - recall_8: 0.6686 - accuracy: 0.7285 - precision_8: 0.7452\n",
      "Epoch 13/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5632 - recall_8: 0.7229 - accuracy: 0.7535 - precision_8: 0.7575\n",
      "Epoch 14/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5434 - recall_8: 0.7171 - accuracy: 0.7645 - precision_8: 0.7795\n",
      "Epoch 15/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5311 - recall_8: 0.7686 - accuracy: 0.7715 - precision_8: 0.7620\n",
      "Epoch 16/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5239 - recall_8: 0.7571 - accuracy: 0.7742 - precision_8: 0.7726\n",
      "Epoch 17/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5093 - recall_8: 0.7486 - accuracy: 0.7756 - precision_8: 0.7798\n",
      "Epoch 18/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4911 - recall_8: 0.7600 - accuracy: 0.7881 - precision_8: 0.7940\n",
      "Epoch 19/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4730 - recall_8: 0.8114 - accuracy: 0.8075 - precision_8: 0.7955\n",
      "Epoch 20/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4817 - recall_8: 0.7629 - accuracy: 0.7978 - precision_8: 0.8091\n",
      "Epoch 21/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4402 - recall_8: 0.8314 - accuracy: 0.8421 - precision_8: 0.8410\n",
      "Epoch 22/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4437 - recall_8: 0.8086 - accuracy: 0.8199 - precision_8: 0.8179\n",
      "Epoch 23/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4423 - recall_8: 0.7971 - accuracy: 0.8172 - precision_8: 0.8206\n",
      "Epoch 24/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4356 - recall_8: 0.8429 - accuracy: 0.8380 - precision_8: 0.8263\n",
      "Epoch 25/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3958 - recall_8: 0.8200 - accuracy: 0.8587 - precision_8: 0.8804\n",
      "Epoch 26/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3823 - recall_8: 0.8514 - accuracy: 0.8615 - precision_8: 0.8613\n",
      "Epoch 27/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3978 - recall_8: 0.8486 - accuracy: 0.8573 - precision_8: 0.8559\n",
      "Epoch 28/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3915 - recall_8: 0.8286 - accuracy: 0.8560 - precision_8: 0.8683\n",
      "Epoch 29/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3571 - recall_8: 0.8600 - accuracy: 0.8850 - precision_8: 0.8985\n",
      "Epoch 30/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3519 - recall_8: 0.8743 - accuracy: 0.8850 - precision_8: 0.8870\n",
      "Epoch 31/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.3436 - recall_8: 0.8743 - accuracy: 0.8947 - precision_8: 0.9053\n",
      "Epoch 32/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3372 - recall_8: 0.8829 - accuracy: 0.8878 - precision_8: 0.8854\n",
      "Epoch 33/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3351 - recall_8: 0.8686 - accuracy: 0.8864 - precision_8: 0.8941\n",
      "Epoch 34/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2896 - recall_8: 0.9057 - accuracy: 0.9183 - precision_8: 0.9242\n",
      "Epoch 35/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2912 - recall_8: 0.9029 - accuracy: 0.9224 - precision_8: 0.9349\n",
      "Epoch 36/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2736 - recall_8: 0.9143 - accuracy: 0.9211 - precision_8: 0.9222\n",
      "Epoch 37/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2768 - recall_8: 0.9029 - accuracy: 0.9155 - precision_8: 0.9213\n",
      "Epoch 38/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2651 - recall_8: 0.9000 - accuracy: 0.9169 - precision_8: 0.9265\n",
      "Epoch 39/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2825 - recall_8: 0.9057 - accuracy: 0.9086 - precision_8: 0.9057\n",
      "Epoch 40/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2828 - recall_8: 0.9057 - accuracy: 0.9114 - precision_8: 0.9109\n",
      "Epoch 41/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2504 - recall_8: 0.9200 - accuracy: 0.9252 - precision_8: 0.9253\n",
      "Epoch 42/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2337 - recall_8: 0.9371 - accuracy: 0.9418 - precision_8: 0.9425\n",
      "Epoch 43/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2365 - recall_8: 0.9400 - accuracy: 0.9404 - precision_8: 0.9373\n",
      "Epoch 44/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2220 - recall_8: 0.9343 - accuracy: 0.9432 - precision_8: 0.9478\n",
      "Epoch 45/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2024 - recall_8: 0.9543 - accuracy: 0.9584 - precision_8: 0.9598\n",
      "Epoch 46/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1972 - recall_8: 0.9514 - accuracy: 0.9598 - precision_8: 0.9652\n",
      "Epoch 47/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1984 - recall_8: 0.9486 - accuracy: 0.9529 - precision_8: 0.9540\n",
      "Epoch 48/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2068 - recall_8: 0.9343 - accuracy: 0.9404 - precision_8: 0.9424\n",
      "Epoch 49/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1774 - recall_8: 0.9600 - accuracy: 0.9640 - precision_8: 0.9655\n",
      "Epoch 50/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1730 - recall_8: 0.9657 - accuracy: 0.9668 - precision_8: 0.9657\n",
      "Epoch 51/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1674 - recall_8: 0.9571 - accuracy: 0.9723 - precision_8: 0.9853\n",
      "Epoch 52/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1630 - recall_8: 0.9800 - accuracy: 0.9778 - precision_8: 0.9744\n",
      "Epoch 53/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1507 - recall_8: 0.9743 - accuracy: 0.9751 - precision_8: 0.9743\n",
      "Epoch 54/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1491 - recall_8: 0.9743 - accuracy: 0.9751 - precision_8: 0.9743\n",
      "Epoch 55/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1529 - recall_8: 0.9714 - accuracy: 0.9737 - precision_8: 0.9742\n",
      "Epoch 56/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1404 - recall_8: 0.9800 - accuracy: 0.9792 - precision_8: 0.9772\n",
      "Epoch 57/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1403 - recall_8: 0.9686 - accuracy: 0.9737 - precision_8: 0.9769\n",
      "Epoch 58/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1273 - recall_8: 0.9829 - accuracy: 0.9834 - precision_8: 0.9829\n",
      "Epoch 59/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1205 - recall_8: 0.9914 - accuracy: 0.9889 - precision_8: 0.9858\n",
      "Epoch 60/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1210 - recall_8: 0.9857 - accuracy: 0.9848 - precision_8: 0.9829\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fc4ace7c730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Score for fold 9: loss of 1.284468650817871; recall_8 of 61.764705181121826% ; accuracy of 66.25000238418579%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 1.6631 - recall_9: 0.3695 - accuracy: 0.5235 - precision_9: 0.4941\n",
      "Epoch 2/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.8482 - recall_9: 0.2111 - accuracy: 0.5706 - precision_9: 0.6372\n",
      "Epoch 3/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.7061 - recall_9: 0.4516 - accuracy: 0.5748 - precision_9: 0.5620\n",
      "Epoch 4/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6846 - recall_9: 0.4018 - accuracy: 0.5956 - precision_9: 0.6089\n",
      "Epoch 5/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6745 - recall_9: 0.5543 - accuracy: 0.6094 - precision_9: 0.5925\n",
      "Epoch 6/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6722 - recall_9: 0.4340 - accuracy: 0.6136 - precision_9: 0.6325\n",
      "Epoch 7/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6608 - recall_9: 0.6041 - accuracy: 0.6440 - precision_9: 0.6280\n",
      "Epoch 8/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6370 - recall_9: 0.5513 - accuracy: 0.6579 - precision_9: 0.6667\n",
      "Epoch 9/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6249 - recall_9: 0.6100 - accuracy: 0.6787 - precision_9: 0.6775\n",
      "Epoch 10/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.6091 - recall_9: 0.6158 - accuracy: 0.6856 - precision_9: 0.6863\n",
      "Epoch 11/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5950 - recall_9: 0.6540 - accuracy: 0.7050 - precision_9: 0.7013\n",
      "Epoch 12/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5672 - recall_9: 0.6657 - accuracy: 0.7382 - precision_9: 0.7517\n",
      "Epoch 13/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5521 - recall_9: 0.7009 - accuracy: 0.7548 - precision_9: 0.7611\n",
      "Epoch 14/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5388 - recall_9: 0.6628 - accuracy: 0.7590 - precision_9: 0.7930\n",
      "Epoch 15/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5315 - recall_9: 0.7243 - accuracy: 0.7632 - precision_9: 0.7623\n",
      "Epoch 16/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.5004 - recall_9: 0.7302 - accuracy: 0.7992 - precision_9: 0.8245\n",
      "Epoch 17/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4689 - recall_9: 0.7683 - accuracy: 0.8227 - precision_9: 0.8424\n",
      "Epoch 18/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4611 - recall_9: 0.7390 - accuracy: 0.8047 - precision_9: 0.8289\n",
      "Epoch 19/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4491 - recall_9: 0.7683 - accuracy: 0.8075 - precision_9: 0.8137\n",
      "Epoch 20/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4205 - recall_9: 0.7859 - accuracy: 0.8380 - precision_9: 0.8590\n",
      "Epoch 21/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.4127 - recall_9: 0.8123 - accuracy: 0.8504 - precision_9: 0.8629\n",
      "Epoch 22/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3903 - recall_9: 0.8182 - accuracy: 0.8657 - precision_9: 0.8885\n",
      "Epoch 23/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3778 - recall_9: 0.8387 - accuracy: 0.8657 - precision_9: 0.8720\n",
      "Epoch 24/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3687 - recall_9: 0.8387 - accuracy: 0.8740 - precision_9: 0.8882\n",
      "Epoch 25/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3916 - recall_9: 0.8152 - accuracy: 0.8587 - precision_9: 0.8770\n",
      "Epoch 26/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3585 - recall_9: 0.8328 - accuracy: 0.8781 - precision_9: 0.9016\n",
      "Epoch 27/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3253 - recall_9: 0.8504 - accuracy: 0.8961 - precision_9: 0.9236\n",
      "Epoch 28/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3223 - recall_9: 0.8534 - accuracy: 0.8878 - precision_9: 0.9037\n",
      "Epoch 29/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.3020 - recall_9: 0.8710 - accuracy: 0.9086 - precision_9: 0.9310\n",
      "Epoch 30/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2995 - recall_9: 0.8915 - accuracy: 0.9141 - precision_9: 0.9240\n",
      "Epoch 31/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2869 - recall_9: 0.8798 - accuracy: 0.9100 - precision_9: 0.9259\n",
      "Epoch 32/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2649 - recall_9: 0.8974 - accuracy: 0.9280 - precision_9: 0.9474\n",
      "Epoch 33/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.2571 - recall_9: 0.9032 - accuracy: 0.9335 - precision_9: 0.9536\n",
      "Epoch 34/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2510 - recall_9: 0.9120 - accuracy: 0.9335 - precision_9: 0.9453\n",
      "Epoch 35/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2419 - recall_9: 0.9062 - accuracy: 0.9280 - precision_9: 0.9392\n",
      "Epoch 36/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2527 - recall_9: 0.8915 - accuracy: 0.9197 - precision_9: 0.9354\n",
      "Epoch 37/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2172 - recall_9: 0.9150 - accuracy: 0.9474 - precision_9: 0.9720\n",
      "Epoch 38/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2247 - recall_9: 0.9238 - accuracy: 0.9391 - precision_9: 0.9459\n",
      "Epoch 39/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2127 - recall_9: 0.9355 - accuracy: 0.9404 - precision_9: 0.9382\n",
      "Epoch 40/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1930 - recall_9: 0.9413 - accuracy: 0.9543 - precision_9: 0.9611\n",
      "Epoch 41/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1960 - recall_9: 0.9267 - accuracy: 0.9488 - precision_9: 0.9634\n",
      "Epoch 42/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1977 - recall_9: 0.9472 - accuracy: 0.9529 - precision_9: 0.9528\n",
      "Epoch 43/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1737 - recall_9: 0.9472 - accuracy: 0.9681 - precision_9: 0.9848\n",
      "Epoch 44/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1744 - recall_9: 0.9531 - accuracy: 0.9654 - precision_9: 0.9731\n",
      "Epoch 45/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1709 - recall_9: 0.9413 - accuracy: 0.9598 - precision_9: 0.9727\n",
      "Epoch 46/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1514 - recall_9: 0.9560 - accuracy: 0.9723 - precision_9: 0.9849\n",
      "Epoch 47/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1461 - recall_9: 0.9648 - accuracy: 0.9737 - precision_9: 0.9792\n",
      "Epoch 48/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1382 - recall_9: 0.9589 - accuracy: 0.9751 - precision_9: 0.9879\n",
      "Epoch 49/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1427 - recall_9: 0.9677 - accuracy: 0.9778 - precision_9: 0.9851\n",
      "Epoch 50/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1405 - recall_9: 0.9677 - accuracy: 0.9737 - precision_9: 0.9763\n",
      "Epoch 51/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1317 - recall_9: 0.9677 - accuracy: 0.9778 - precision_9: 0.9851\n",
      "Epoch 52/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1257 - recall_9: 0.9736 - accuracy: 0.9792 - precision_9: 0.9822\n",
      "Epoch 53/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1182 - recall_9: 0.9765 - accuracy: 0.9806 - precision_9: 0.9823\n",
      "Epoch 54/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1165 - recall_9: 0.9648 - accuracy: 0.9765 - precision_9: 0.9850\n",
      "Epoch 55/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1116 - recall_9: 0.9677 - accuracy: 0.9806 - precision_9: 0.9910\n",
      "Epoch 56/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1099 - recall_9: 0.9795 - accuracy: 0.9875 - precision_9: 0.9940\n",
      "Epoch 57/60\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1070 - recall_9: 0.9765 - accuracy: 0.9848 - precision_9: 0.9911\n",
      "Epoch 58/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0942 - recall_9: 0.9795 - accuracy: 0.9875 - precision_9: 0.9940\n",
      "Epoch 59/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0890 - recall_9: 0.9853 - accuracy: 0.9903 - precision_9: 0.9941\n",
      "Epoch 60/60\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.0849 - recall_9: 0.9941 - accuracy: 0.9958 - precision_9: 0.9971\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fc4b22eeea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Score for fold 10: loss of 1.6976913213729858; recall_9 of 65.11628031730652% ; accuracy of 57.499998807907104%\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 1.1780860424041748 - Accuracy: 69.13580298423767%  - Recall: 0.7142857313156128% - Precision: 0.625%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 1.3665492534637451 - Accuracy: 55.55555820465088%  - Recall: 0.6176470518112183% - Precision: 0.47727271914482117%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 1.4485677480697632 - Accuracy: 62.5%  - Recall: 0.6216216087341309% - Precision: 0.5897436141967773%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 1.8024698495864868 - Accuracy: 60.00000238418579%  - Recall: 0.44999998807907104% - Precision: 0.6428571343421936%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 1.949022650718689 - Accuracy: 64.99999761581421%  - Recall: 0.7027027010917664% - Precision: 0.604651153087616%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 1.3497289419174194 - Accuracy: 60.00000238418579%  - Recall: 0.6818181872367859% - Precision: 0.625%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 1.0444746017456055 - Accuracy: 68.75%  - Recall: 0.6666666865348816% - Precision: 0.7179487347602844%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 1.268777847290039 - Accuracy: 63.749998807907104%  - Recall: 0.5526315569877625% - Precision: 0.6363636255264282%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 1.284468650817871 - Accuracy: 66.25000238418579%  - Recall: 0.6176470518112183% - Precision: 0.6000000238418579%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 1.6976913213729858 - Accuracy: 57.499998807907104%  - Recall: 0.6511628031730652% - Precision: 0.5957446694374084%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 62.844136357307434 (+- 4.3533015727115645)\n",
      "> Recall: 0.6276183366775513 (+- 0.0745464600631801)\n",
      "> Precision: 0.6114581674337387 (+- 0.05675823748734591)\n",
      "> Loss: 1.438983690738678\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "fold_no = 1\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    model = Sequential()\n",
    "    #model.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(256,input_dim=768,activation='relu'))\n",
    "    #model.add(Dropout(rate=0.5))\n",
    "    model.add(Dense(20,activation='relu',kernel_regularizer=l2(0.05), bias_regularizer=l2(0.01)))\n",
    "    model.add(Dense(10,activation='relu'))\n",
    "    #model.add(Dropout(rate=0.4))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    #model.summary()\n",
    "    \n",
    "    AD = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    model.compile(optimizer=AD,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[ keras.metrics.Recall(),'accuracy',keras.metrics.Precision()])\n",
    "    \n",
    "\n",
    "    \n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    \n",
    "    #Training LSTM model\n",
    "    history = model.fit(inputs[train], targets[train],\n",
    "          batch_size=10,\n",
    "          epochs=60,verbose=1\n",
    "          )\n",
    "\n",
    "\n",
    "\n",
    "    #Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% ; {model.metrics_names[2]} of {scores[2]*100}%')\n",
    "    acc_per_fold.append(scores[2] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    rec_per_fold.append(scores[1])\n",
    "    per_per_fold.append(scores[3])\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%  - Recall: {rec_per_fold[i]}% - Precision: {per_per_fold[i]}%')\n",
    "\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Recall: {np.mean(rec_per_fold)} (+- {np.std(rec_per_fold)})')\n",
    "print(f'> Precision: {np.mean(per_per_fold)} (+- {np.std(per_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "HO_ngOFSVDtz",
    "outputId": "e1d5777a-27b1-49ab-dc27-1b672500b806"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69.14, 55.56, 62.5, 60.0, 65.0, 60.0, 68.75, 63.75, 66.25, 57.5]\n",
      "62.845000000000006\n",
      "4.353164940592074\n",
      "[71.43, 61.76, 62.16, 45.0, 70.27, 68.18, 66.67, 55.26, 61.76, 65.12]\n",
      "62.761\n",
      "7.455405354506221\n"
     ]
    }
   ],
   "source": [
    "# List of 10 accuracyand recall scores\n",
    "acc_per_fold\n",
    "\n",
    "round_to_tenths = [round(num, 2) for num in acc_per_fold]\n",
    "print(round_to_tenths)\n",
    "print(np.mean(round_to_tenths))\n",
    "print(np.std(round_to_tenths))\n",
    "\n",
    "rec_per_fold\n",
    "\n",
    "round_to_tenths_rec = [round(num*100, 2) for num in rec_per_fold]\n",
    "print(round_to_tenths_rec)\n",
    "print(np.mean(round_to_tenths_rec))\n",
    "print(np.std(round_to_tenths_rec))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Variant2_BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
