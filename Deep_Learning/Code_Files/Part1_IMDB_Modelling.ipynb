{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QeaADjtMNn6P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandasql in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (0.7.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pandasql) (0.24.2)\n",
      "Requirement already satisfied: sqlalchemy in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pandasql) (1.2.11)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pandasql) (1.16.4)\n",
      "Requirement already satisfied: pytz>=2011k in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pandas->pandasql) (2018.4)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pandas->pandasql) (2.7.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas->pandasql) (1.11.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#####################   PART 1  ########################\n",
    "\n",
    "!pip install pandasql\n",
    "import pandasql as ps\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Embedding, Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yUVboRtKJrMp",
    "outputId": "e5467126-bfc6-415d-e8a9-d8294592a6ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "#Dataset load and prepare input and target labels, make train test split\n",
    "X_Y_full = pd.read_csv('Dataset.csv')\n",
    "print(len(X_Y_full))\n",
    "#X_Y_full = ps.sqldf(\"select * from X_Y_full order by review desc limit 20000\")\n",
    "X_Y_full['review']=X_Y_full['review'].str.replace('<br />', '').replace('[^a-zA-Z]', '')\n",
    "train, validate, test = np.split(X_Y_full.sample(frac=1), [int(.5*len(X_Y_full)), int(.8*len(X_Y_full))])\n",
    "x_train = (train.review) \n",
    "y_train = (train.sentiment)\n",
    "x_validate = (validate.review)\n",
    "y_validate = (validate.sentiment)\n",
    "x_test = (test.review )\n",
    "y_test = (test.sentiment)\n",
    "maxLen = 1000#len(max(X_Y_full.review, key=len).split(' '))#  maximum length of sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cvafX0c-Cf0c"
   },
   "outputs": [],
   "source": [
    "#Tokenise the sequences and give indices to them\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=1000,filters='![0-9]\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=False)\n",
    "tokenizer.fit_on_texts(X_Y_full['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Q-MRdBkCI_eG",
    "outputId": "4f1c30ca-235b-4c23-d063-19dd620fcd4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155554"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenise train test and validation\n",
    "unique_vocab = len(tokenizer.word_index)\n",
    "unique_vocab\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "#print(x_train[0])\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxLen)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxLen)\n",
    "x_validate = tokenizer.texts_to_sequences(x_validate)\n",
    "x_validate = sequence.pad_sequences(x_validate, maxlen=maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s5NQWfYXNn6i"
   },
   "outputs": [],
   "source": [
    "#Convert target to 0 and 1 and do numpy transformations to leverage fast parallel processing\n",
    "y_train_new = ps.sqldf(\"select case when sentiment='positive' then 1 else 0 end as sentiment_new from train\")\n",
    "y_test_new = ps.sqldf(\"select case when sentiment='positive' then 1 else 0 end as sentiment_new from test\")\n",
    "y_validation_new = ps.sqldf(\"select case when sentiment='positive' then 1 else 0 end as sentiment_new from validate\")\n",
    "y_train_new = np.asarray(y_train_new['sentiment_new'])\n",
    "y_test_new = np.asarray(y_test_new['sentiment_new'])\n",
    "y_validation_new = np.asarray(y_validation_new['sentiment_new'])\n",
    "x_train = np.asarray(x_train)\n",
    "y_train_new = np.asarray(y_train_new)\n",
    "x_validate = np.asarray(x_validate)\n",
    "y_validation_new = np.asarray(y_validation_new)\n",
    "x_test = np.asarray(x_test)\n",
    "y_test_new = np.asarray(y_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "PeU8yd7mNn7Z",
    "outputId": "6da16673-67fa-4670-86d6-77995f35d377"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_30 (Embedding)     (500, None, 50)           50000     \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (500, None, 100)          60400     \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (500, 100)                80400     \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (500, 100)                10100     \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (500, 100)                0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (500, 1)                  101       \n",
      "=================================================================\n",
      "Total params: 201,001\n",
      "Trainable params: 201,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model 1 - Simple RNN\n",
    "model = Sequential()\n",
    "embedding_size=50\n",
    "model.add(Embedding(1000,\n",
    "                    embedding_size,\n",
    "                    input_length=maxLen,batch_input_shape=[500, None]))\n",
    "model.add(layers.SimpleRNN(100,return_sequences=False))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "58LdnXyFNn7b"
   },
   "outputs": [],
   "source": [
    "#Loss Function # tried different values for learning_rate\n",
    "from tensorflow.keras import optimizers\n",
    "AD = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(optimizer=AD,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JWA_qBEev2j4",
    "outputId": "c59c78f2-ea3b-415e-d682-d8577dfbf604"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory not used yet.\n"
     ]
    }
   ],
   "source": [
    "#Checkpointing\n",
    "import os\n",
    "checkpoint_dir = 'bestt_2_1'\n",
    "import shutil\n",
    "try:\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "except:\n",
    "    print(\"directory not used yet.\")\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_\")\n",
    "\n",
    "cb=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    monitor='loss',\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uzHrTjz01Xpr"
   },
   "outputs": [],
   "source": [
    "#Early stopping # Patience values experimented for different values\n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training SimpleRNN\n",
    "hist = model.fit(x_train, y_train_new,\n",
    "          batch_size=500,\n",
    "          epochs=100,callbacks=[cb],verbose=1,validation_data=(x_validate, y_validation_new)\n",
    "          )\n",
    "model.save('Part_A_Best_2_50k_SimpleRNN.h5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_32 (Embedding)     (1, None, 50)             50000     \n",
      "_________________________________________________________________\n",
      "lstm_28 (LSTM)               (1, None, 100)            60400     \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (1, 100)                  80400     \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (1, 100)                  10100     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (1, 100)                  0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (1, 1)                    101       \n",
      "=================================================================\n",
      "Total params: 201,001\n",
      "Trainable params: 201,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model 1 Testing with custom examples so batch size as 1\n",
    "model = Sequential()\n",
    "embedding_size=50\n",
    "model.add(Embedding(1000,\n",
    "                    embedding_size,\n",
    "                    input_length=maxLen,batch_input_shape=[1, None]))\n",
    "model.add(layers.SimpleRNN(100,return_sequences=False))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.load_weights(tf.train.latest_checkpoint('bestt_2_1'))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "#custom examples\n",
    "Xnew = np.array([\"This movie should have ended as soon as the joke about Bebe's Kids is told in the opening. I liked Robin Harris and most of his comedy but the really funny routines were not meant to be something the whole family could go see. Liken it to taking the point of one of Jerry Seinfeld's jokes and attempting to squeeze the joke for as long as you can in order to turn it into a movie. This movie had to be self-serving because I can not find anyone who found the settings or antics familiar. Whats most funny about this movie is the gumption of the writers and producers to pass it off as something of value. 1/10 stars because there is no half star.\"])\n",
    "Xnew = np.array([\"poor expectation but good result\"])\n",
    "X1new_indices = tokenizer.texts_to_sequences(Xnew)\n",
    "X1new_indices = sequence.pad_sequences(X1new_indices, maxlen=maxLen)\n",
    "ynew = model.predict_classes(X1new_indices,batch_size=1)\n",
    "ynew[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking and testing load_model functionality\n",
    "import tensorflow as tf \n",
    "classifierLoad = tf.keras.models.load_model('Part_A_Best_2_50k.h5')\n",
    "loss, acc = classifierLoad.evaluate(x_test, y_test_new)\n",
    "loss, acc = classifierLoad.evaluate(x_train, y_train_new)\n",
    "print(\"Test accuracy = \", acc)\n",
    "loss, acc = classifierLoad.evaluate(x_validate, y_validation_new)\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2 - 1 LSTM\n",
    "model = Sequential()\n",
    "embedding_size=50\n",
    "model.add(Embedding(1000,\n",
    "                    embedding_size,\n",
    "                    input_length=maxLen,batch_input_shape=[500, None]))\n",
    "model.add(tf.keras.layers.LSTM(100,return_sequences=False, stateful=False,recurrent_initializer='glorot_uniform'))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss Function # tried different values for learning_rate\n",
    "from tensorflow.keras import optimizers\n",
    "AD = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(optimizer=AD,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checkpointing\n",
    "import os\n",
    "checkpoint_dir = 'bestt_2_simpleRNN'\n",
    "import shutil\n",
    "try:\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "except:\n",
    "    print(\"directory not used yet.\")\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_\")\n",
    "\n",
    "cb=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    monitor='loss',\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Early stopping # Patience values experimented for different values\n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=7) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training LSTM model\n",
    "hist = model.fit(x_train, y_train_new,\n",
    "          batch_size=500,\n",
    "          epochs=100,callbacks=[cb,es],verbose=1,validation_data=(x_validate, y_validation_new)\n",
    "          )\n",
    "model.save('Part_A_Best_2_50k_LSTM.h5')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Model 2 Testing with custom examples so batch size as 1\n",
    "model = Sequential()\n",
    "embedding_size=50\n",
    "model.add(Embedding(1000,\n",
    "                    embedding_size,\n",
    "                    input_length=maxLen,batch_input_shape=[1, None]))\n",
    "model.add(tf.keras.layers.LSTM(100,return_sequences=False, stateful=True,recurrent_initializer='glorot_uniform'))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.load_weights(tf.train.latest_checkpoint('bestt_2_LSTM'))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "#custom examples\n",
    "Xnew = np.array([\"This movie should have ended as soon as the joke about Bebe's Kids is told in the opening. I liked Robin Harris and most of his comedy but the really funny routines were not meant to be something the whole family could go see. Liken it to taking the point of one of Jerry Seinfeld's jokes and attempting to squeeze the joke for as long as you can in order to turn it into a movie. This movie had to be self-serving because I can not find anyone who found the settings or antics familiar. Whats most funny about this movie is the gumption of the writers and producers to pass it off as something of value. 1/10 stars because there is no half star.\"])\n",
    "Xnew = np.array([\"poor expectation but good result\"])\n",
    "X1new_indices = tokenizer.texts_to_sequences(Xnew)\n",
    "X1new_indices = sequence.pad_sequences(X1new_indices, maxlen=maxLen)\n",
    "ynew = model.predict_classes(X1new_indices,batch_size=1)\n",
    "ynew[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 3 - Stacked LSTM -2 LSTMs\n",
    "model = Sequential()\n",
    "embedding_size=50\n",
    "model.add(Embedding(1000,\n",
    "                    embedding_size,\n",
    "                    input_length=maxLen,batch_input_shape=[500, None]))\n",
    "model.add(tf.keras.layers.LSTM(100,return_sequences=True,  stateful=True,recurrent_initializer='glorot_uniform'))\n",
    "model.add(tf.keras.layers.LSTM(100,return_sequences=False, stateful=True,recurrent_initializer='glorot_uniform'))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss Function # tried different values for learning_rate\n",
    "from tensorflow.keras import optimizers\n",
    "AD = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(optimizer=AD,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checkpointing\n",
    "import os\n",
    "checkpoint_dir = 'bestt_2_2LSTM'\n",
    "import shutil\n",
    "try:\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "except:\n",
    "    print(\"directory not used yet.\")\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_\")\n",
    "\n",
    "cb=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    monitor='loss',\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Early stopping # Patience values experimented for different values\n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "es = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=7) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training 2 LSTM model\n",
    "hist = model.fit(x_train, y_train_new,\n",
    "          batch_size=500,\n",
    "          epochs=100,callbacks=[cb,es],verbose=1,validation_data=(x_validate, y_validation_new)\n",
    "          )\n",
    "model.save('Part_A_Best_2_50k_2LSTM.h5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 3 Testing with custom examples so batch size as 1\n",
    "model = Sequential()\n",
    "embedding_size=50\n",
    "model.add(Embedding(1000,\n",
    "                    embedding_size,\n",
    "                    input_length=maxLen,batch_input_shape=[1, None]))\n",
    "model.add(tf.keras.layers.LSTM(100,return_sequences=False, stateful=True,recurrent_initializer='glorot_uniform'))\n",
    "model.add(tf.keras.layers.LSTM(100,return_sequences=False, stateful=True,recurrent_initializer='glorot_uniform'))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.load_weights(tf.train.latest_checkpoint('bestt_2_1'))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "#custom examples\n",
    "Xnew = np.array([\"This movie should have ended as soon as the joke about Bebe's Kids is told in the opening. I liked Robin Harris and most of his comedy but the really funny routines were not meant to be something the whole family could go see. Liken it to taking the point of one of Jerry Seinfeld's jokes and attempting to squeeze the joke for as long as you can in order to turn it into a movie. This movie had to be self-serving because I can not find anyone who found the settings or antics familiar. Whats most funny about this movie is the gumption of the writers and producers to pass it off as something of value. 1/10 stars because there is no half star.\"])\n",
    "Xnew = np.array([\"poor expectation but good result\"])\n",
    "X1new_indices = tokenizer.texts_to_sequences(Xnew)\n",
    "X1new_indices = sequence.pad_sequences(X1new_indices, maxlen=maxLen)\n",
    "ynew = model.predict_classes(X1new_indices,batch_size=1)\n",
    "ynew[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################Pretrained Embeddings#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shaping the embedding vector from the file\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAking Text<->index <-> Vector pairs\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom created pretrained_embedding_layer\n",
    "\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  \n",
    "    emb_dim = word_to_vec_map[\"movie\"].shape[0]     \n",
    "    emb_matrix = np.full([1000, emb_dim], 0, dtype = float) \n",
    "    for word, idx in word_to_index.items():\n",
    "        if(idx<1000):\n",
    "            emb_matrix[idx, :] = word_to_vec_map[word]\n",
    "    embedding_layer = Embedding(1000,emb_dim,weights=[emb_matrix],trainable=False)\n",
    "    embedding_layer.build((None,)) \n",
    "    return embedding_layer\n",
    "embeddings = pretrained_embedding_layer(word_to_vec_map, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Extra modules\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, LSTM, Activation,SimpleRNN,concatenate,Flatten,MaxPooling1D\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model for the pretrained embeddings and the rest of the architecture\n",
    "def pretrained_moedl(input_shape, word_to_vec_map, word_to_index):\n",
    "   \n",
    "    raw_inputs = Input(shape=input_shape, dtype='int32')\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    embeddings = embedding_layer(raw_inputs)   \n",
    "    X = SimpleRNN(units = 100)(embeddings)\n",
    "    Y = Dropout(rate = 0.5 )(X)\n",
    "    B = Dense(units = 100)(Y)\n",
    "    B = Dense(units = 1)(B)\n",
    "    C = Activation(\"sigmoid\")(B)\n",
    "    model = Model(inputs=raw_inputs, outputs=C)\n",
    "    return model\n",
    "model_pre = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)\n",
    "model_pre.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function\n",
    "model_pre.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory not used yet.\n"
     ]
    }
   ],
   "source": [
    "#checkpointing\n",
    "import os\n",
    "checkpoint_dir = 'trained embed'\n",
    "import shutil\n",
    "try:\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "except:\n",
    "    print(\"directory not used yet.\")\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_\")\n",
    "\n",
    "cb=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    monitor='loss',\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model training\n",
    "model_pre.fit(x_train, y_train_new, epochs = 50, batch_size = 1000, callbacks=[cb,es],shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 8s 2ms/sample - loss: 1.0732 - acc: 0.6545\n",
      "Test accuracy =  0.6545\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 0.1804 - acc: 0.9302\n",
      "Test accuracy =  0.9302\n",
      "6000/6000 [==============================] - 12s 2ms/sample - loss: 1.0544 - acc: 0.6603\n",
      "Test accuracy =  0.66033334\n"
     ]
    }
   ],
   "source": [
    "#Model Evaluation\n",
    "loss, acc = model_pre.evaluate(x_test, y_test_new)\n",
    "#print()\n",
    "print(\"Test accuracy = \", acc)\n",
    "\n",
    "loss, acc = model_pre.evaluate(x_train, y_train_new)\n",
    "#print()\n",
    "print(\"Test accuracy = \", acc)\n",
    "\n",
    "loss, acc = model_pre.evaluate(x_validate, y_validation_new)\n",
    "#print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jCIcu-JWNn7k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_44 (InputLayer)           [(None, 1000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_45 (InputLayer)           [(None, 1000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_46 (InputLayer)           [(None, 1000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_50 (Embedding)        (None, 1000, 50)     50000       input_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_51 (Embedding)        (None, 1000, 50)     50000       input_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_52 (Embedding)        (None, 1000, 50)     50000       input_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 1000, 100)    1500100     embedding_50[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 1000, 100)    3000100     embedding_51[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 1000, 100)    4500100     embedding_52[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 500, 100)     0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 500, 100)     0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 500, 100)     0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 500, 300)     0           max_pooling1d_16[0][0]           \n",
      "                                                                 max_pooling1d_17[0][0]           \n",
      "                                                                 max_pooling1d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 50)           70200       concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 50)           0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 1)            51          dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 1)            0           dense_32[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 9,220,551\n",
      "Trainable params: 9,070,551\n",
      "Non-trainable params: 150,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#CNN - as a replacement for LSTM\n",
    "raw_inputs = Input(shape=(1000,), dtype='int32')\n",
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "embeddings = embedding_layer(raw_inputs)  \n",
    "#heterogeneous kernel sized convolution layers\n",
    "\n",
    "x1 = Conv1D(100,\n",
    "                        300,\n",
    "                        padding='same',\n",
    "                        activation='relu',\n",
    "                        strides=1)(embeddings) \n",
    "x4=MaxPooling1D()(x1)\n",
    "\n",
    "raw_inputs1 = Input(shape=(1000,), dtype='int32')\n",
    "embedding_layer1 = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "embeddings1 = embedding_layer1(raw_inputs1) \n",
    "#Experimented with 3 5 & 7 kernel sizes also\n",
    "x2 = Conv1D(100,\n",
    "                        600,\n",
    "                        padding='same',\n",
    "                        activation='relu',\n",
    "                        strides=1)(embeddings1) \n",
    "x3 = MaxPooling1D()(x2)\n",
    "\n",
    "raw_inputs2 = Input(shape=(1000,), dtype='int32')\n",
    "embedding_layer2 = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "embeddings2 = embedding_layer2(raw_inputs2) \n",
    "\n",
    "x5 = Conv1D(100,\n",
    "                        900,\n",
    "                        padding='same',\n",
    "                        activation='relu',\n",
    "                        strides=1)(embeddings2) \n",
    "x7 = MaxPooling1D()(x5)\n",
    "\n",
    "\n",
    "summ = concatenate([x4,x3,x7])\n",
    "f = Flatten()(summ)\n",
    "#L = LSTM(50, return_sequences=False)(summ)\n",
    "DR = Dropout(rate=0.5)(f)\n",
    "B = Dense(units = 1)(DR)\n",
    "    # Add a softmax activation\n",
    "C = Activation(\"sigmoid\")(B)\n",
    "\n",
    "model_con = Model(inputs = [raw_inputs,raw_inputs1,raw_inputs2],outputs = C)\n",
    "model_con.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 6000 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 468s 47ms/sample - loss: 0.7285 - acc: 0.5010 - val_loss: 0.6928 - val_acc: 0.5210\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 466s 47ms/sample - loss: 0.6975 - acc: 0.5322 - val_loss: 0.6894 - val_acc: 0.5367\n",
      "Epoch 3/50\n",
      " 9000/10000 [==========================>...] - ETA: 34s - loss: 0.6786 - acc: 0.5662 "
     ]
    }
   ],
   "source": [
    "#Loss function & train\n",
    "model_con.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_con.fit([x_train,x_train,x_train], y_train_new,batch_size = 1000, epochs = 50, shuffle=True,validation_data=([x_validate,x_validate,x_validate], y_validation_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN before LSTM\n",
    "raw_inputs = Input(shape=(1000,), dtype='int32')\n",
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "embeddings = embedding_layer(raw_inputs)  \n",
    "\n",
    "\n",
    "x1 = Conv1D(100,\n",
    "                        300,\n",
    "                        padding='same',\n",
    "                        activation='relu',\n",
    "                        strides=1)(embeddings) \n",
    "x4=MaxPooling1D()(x1)\n",
    "\n",
    "raw_inputs1 = Input(shape=(1000,), dtype='int32')\n",
    "embedding_layer1 = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "embeddings1 = embedding_layer1(raw_inputs1) \n",
    "#Experimented with 3 5 & 7 kernel sizes also\n",
    "x2 = Conv1D(100,\n",
    "                        600,\n",
    "                        padding='same',\n",
    "                        activation='relu',\n",
    "                        strides=1)(embeddings1) \n",
    "x3 = MaxPooling1D()(x2)\n",
    "\n",
    "raw_inputs2 = Input(shape=(1000,), dtype='int32')\n",
    "embedding_layer2 = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "embeddings2 = embedding_layer2(raw_inputs2) \n",
    "\n",
    "x5 = Conv1D(100,\n",
    "                        900,\n",
    "                        padding='same',\n",
    "                        activation='relu',\n",
    "                        strides=1)(embeddings2) \n",
    "x7 = MaxPooling1D()(x5)\n",
    "\n",
    "\n",
    "summ = concatenate([x4,x3,x7])\n",
    "#f = Flatten()(summ)\n",
    "L = LSTM(50, return_sequences=False)(summ)\n",
    "DR = Dropout(rate=0.5)(L)\n",
    "B = Dense(units = 1)(DR)\n",
    "    # Add a softmax activation\n",
    "C = Activation(\"sigmoid\")(B)\n",
    "#summ = keras.layers.add([x1, x2]) \n",
    "model_con = Model(inputs = [raw_inputs,raw_inputs1,raw_inputs2],outputs = C)\n",
    "model_con.summary()\n",
    "\n",
    "#Loss function & train\n",
    "model_con.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_con.fit([x_train,x_train,x_train], y_train_new,batch_size = 1000, epochs = 50, shuffle=True,validation_data=([x_validate,x_validate,x_validate], y_validation_new))\n",
    "\n",
    "#model saving\n",
    "model_con.save('conv_for_LSTM.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "part_A.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
