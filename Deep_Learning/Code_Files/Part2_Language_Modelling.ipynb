{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R5xQFCOCTaWf"
   },
   "outputs": [],
   "source": [
    "############################################3 PArt 2 - Language Modelling ###############################3\n",
    "\n",
    "#Importing needed modules\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pandasql as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "faZ7iSuKF1qf"
   },
   "outputs": [],
   "source": [
    "#Dataset Fetch and creating huge Positive, negative and Mixed strings\n",
    "X_Y_full = pd.read_csv('/content/drive/My Drive/IMDB Dataset.csv')\n",
    "len(X_Y_full)\n",
    "positive =''\n",
    "negative =''\n",
    "mixed = ''\n",
    "#23156374\n",
    "def final_pop(x):\n",
    "    global positive ;\n",
    "    global negative ;\n",
    "    global mixed;\n",
    "    if(len(positive)==3156374):    ###### experimented with larger values and smaller values based on the resources of computation available\n",
    "      return 1\n",
    "    else:\n",
    "      if(x['sentiment'] == 'positive'):\n",
    "        positive=positive+x['review']\n",
    "      else:\n",
    "        negative=negative+x['review']\n",
    "    mixed = mixed+x['review']\n",
    "X_Y_full.apply (lambda row: final_pop(row), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7_uOxED1I816"
   },
   "outputs": [],
   "source": [
    "#sampling from the entire set\n",
    "print(len(positive))\n",
    "extract = 0.7 # tested different values\n",
    "#text = text.lower()\n",
    "text = positive[:int(extract*len(positive))] \n",
    "## change to negative/mixed for other models\n",
    "## Kept as positive as it is the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "1DrwKv5qJCqH",
    "outputId": "21ac7388-cb66-46b9-b66f-bf812e30b1ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\x08', '\\t', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x80', '\\x84', '\\x85', '\\x91', '\\x95', '\\x96', '\\x97', '\\x9a', '\\xa0', '¡', '£', '¨', '«', '®', '´', '·', '»', '½', '¿', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ó', 'ô', 'õ', 'ö', 'ø', 'ú', 'ü', 'ý', 'ğ', 'ı', 'ō', '–', '‘', '’', '“', '”', '…']\n",
      "14423292\n",
      "14423292\n"
     ]
    }
   ],
   "source": [
    "#Dataset preparation with index to characters, characters to index\n",
    "dict_1 = sorted(set(text.lower()))\n",
    "print(dict_1)\n",
    "print(len(text))\n",
    "\n",
    "# Create mappings between vocab and numeric indices\n",
    "char2idx_dict = {u:i for i, u in enumerate(dict_1)}\n",
    "idx2char_dict = np.array(dict_1)\n",
    "\n",
    "# Map all the training text over to a numeric representation\n",
    "text_as_int_dict = np.array([char2idx_dict[c] for c in text.lower()])\n",
    "#print(dict_1)\n",
    "#print(idx2char_dict)\n",
    "print(len(text_as_int_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uuBCA2VqJDAQ",
    "outputId": "626bf193-7b80-44d3-b125-87db4dfd865f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159804\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "print(examples_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cROWSARBJC87"
   },
   "outputs": [],
   "source": [
    "# Create Dataset with sequences of proper sequence length\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int_dict)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nH10a_5sKM2I",
    "outputId": "50204075-dace-41dc-9e94-2bae2788964b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "#Dataset input as word at t, target as word at t+1\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZrBOpwbNKQzy"
   },
   "outputs": [],
   "source": [
    "#Split Dataset into batches\n",
    "BATCH_SIZE = 134\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qPWsEqDLKRpz"
   },
   "outputs": [],
   "source": [
    "#Model Design\n",
    "embedding_dim = 50\n",
    "rnn_units = 2048\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(len(dict_1), embedding_dim,\n",
    "                          batch_input_shape=[BATCH_SIZE, None]))\n",
    "\n",
    "model.add(layers.SimpleRNN(rnn_units,return_sequences=True))\n",
    "model.add(tf.keras.layers.Dense(len(dict_1)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jUwnIipfKhOT"
   },
   "outputs": [],
   "source": [
    "#Define Loss Function\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mqA57hH8K33s",
    "outputId": "c498a329-50e9-45e9-e0d5-5b5881d95794"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory not used yet.\n"
     ]
    }
   ],
   "source": [
    "#Checkpointing - saving model weights\n",
    "import os\n",
    "checkpoint_dir = '/content/drive/My Drive/check_rnn_lm'\n",
    "import shutil\n",
    "try:\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "except:\n",
    "    print(\"directory not used yet.\")\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    monitor='loss',\n",
    "    save_weights_only=True, \n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "6D_z574EKhiv",
    "outputId": "5a2d4079-f021-4076-b451-9ea4b6a677ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1192/1192 [==============================] - 144s 121ms/step - loss: 2.3596\n",
      "Epoch 2/2\n",
      "1192/1192 [==============================] - 144s 121ms/step - loss: 1.8058\n",
      "training complete.\n"
     ]
    }
   ],
   "source": [
    "#Model Training\n",
    "history = model.fit(dataset, epochs=5,callbacks=[checkpoint_callback],verbose=1)\n",
    "print(\"training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hUSmI4iJK8vY"
   },
   "outputs": [],
   "source": [
    "#LOading the saved weights for testing the model with novel sampling so we give batch size as 1 here\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(len(dict_1), embedding_dim,\n",
    "                          batch_input_shape=[1, None]))\n",
    "model.add(layers.SimpleRNN(rnn_units,return_sequences=True))\n",
    "model.add(tf.keras.layers.Dense(len(dict_1)))\n",
    "model.add(layers.SimpleRNN(rnn_units,return_sequences=True))\n",
    "model.add(tf.keras.layers.Dense(len(dict_1)))\n",
    "model.load_weights(tf.train.latest_checkpoint('/content/drive/My Drive/check_rnn_lm'))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lm3cHBfwKhvK"
   },
   "outputs": [],
   "source": [
    "#generate text function which samples out sentences\n",
    "def generate_text(model, start_string):\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 200\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx_dict[s.lower()] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    text_ids_gen = []\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        #print(predictions)\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        #print(predicted_id)\n",
    "\n",
    "        # We pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_ids_gen.append(predicted_id)\n",
    "        text_generated.append(idx2char_dict[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "6RGv-G3zKh4P",
    "outputId": "5e4ffb2e-9e94-44f6-8881-34c91034ea5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The powat cones ort bus omothaunseas mendof d b arnce e orofo a. s pa />br d. we thecon hesenda r atspllie ting cls a te totorngr--lle on, by ang>t cruley. thodan fusecama re anghas hearemavekispeverowa h\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=\"The \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "XmiyuXvHjeZR",
    "outputId": "5a16130d-a1cf-48bb-c05c-10eabcdc58dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (134, None, 50)           6350      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (134, None, 2048)         17195008  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (134, None, 127)          260223    \n",
      "=================================================================\n",
      "Total params: 17,461,581\n",
      "Trainable params: 17,461,581\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Applying LSTM for the model\n",
    "m = tf.keras.Sequential()\n",
    "m.add(tf.keras.layers.Embedding(len(dict_1), embedding_dim,\n",
    "                          batch_input_shape=[BATCH_SIZE, None]))\n",
    "m.add(tf.keras.layers.LSTM(rnn_units,\n",
    "                    return_sequences=True,\n",
    "                    stateful=True,\n",
    "                    recurrent_initializer='glorot_uniform'))\n",
    "m.add(tf.keras.layers.Dense(len(dict_1)))\n",
    "\n",
    "m.summary()\n",
    "\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "m.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = '/content/drive/My Drive/check_lstm_LM'\n",
    "import shutil\n",
    "try:\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "except:\n",
    "    print(\"directory not used yet.\")\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_\")\n",
    "\n",
    "cb=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    monitor='loss',\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "by9gZ_3kkm7l",
    "outputId": "5c8e9538-c660-4774-8415-b15645a7da86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1192/1192 [==============================] - 353s 296ms/step - loss: 1.8215\n",
      "Epoch 2/5\n",
      "1192/1192 [==============================] - 353s 296ms/step - loss: 1.3033\n",
      "Epoch 3/5\n",
      "1192/1192 [==============================] - 353s 296ms/step - loss: 1.2230\n",
      "Epoch 4/5\n",
      "1192/1192 [==============================] - 353s 296ms/step - loss: 1.1775\n",
      "Epoch 5/5\n",
      "1192/1192 [==============================] - 353s 296ms/step - loss: 1.1441\n",
      "training complete.\n"
     ]
    }
   ],
   "source": [
    "#Training LSTM model and saving it\n",
    "history = m.fit(dataset, epochs=5,callbacks=[cb],verbose=1)\n",
    "print(\"training complete.\")\n",
    "m.save('/content/drive/My Drive/Language_Modeling_positiveReviews.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A0ZWx5jAkDKQ"
   },
   "outputs": [],
   "source": [
    "#Loading the saved weights to test the model with batch size as 1\n",
    "model_lstm = tf.keras.Sequential()\n",
    "model_lstm.add(tf.keras.layers.Embedding(len(dict_1), 50,\n",
    "                          batch_input_shape=[1, None]))\n",
    "model_lstm.add(tf.keras.layers.LSTM(2048,\n",
    "                    return_sequences=True,\n",
    "                    stateful=True,\n",
    "                    recurrent_initializer='glorot_uniform'))\n",
    "model_lstm.add(tf.keras.layers.Dense(len(dict_1)))\n",
    "\n",
    "model_lstm.load_weights(tf.train.latest_checkpoint('/content/drive/My Drive/check_lstm_LM+ve'))\n",
    "model_lstm.build(tf.TensorShape([1, None]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "T5Ez7YC47yyA",
    "outputId": "23adcbb9-0e9b-4046-9600-e6568a73b0b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad how the boy wes david dead as allowing day -- freedom. maxline was delivered at times for in a white catwoman (givin shaw-mal) maker a surprise match. streep in being in over all: in any friend as he \n"
     ]
    }
   ],
   "source": [
    "#generating text from the model loaded with saved weights\n",
    "print(generate_text(model_lstm, start_string=u\"bad \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tc-A5uugWXfa",
    "outputId": "183a73e9-e7eb-406b-ddd8-87230961fc5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "#Another method of loading the trained model instead of loading the weights\n",
    "import tensorflow as tf \n",
    "classifierLoad = tf.keras.models.load_model('/content/drive/My Drive/Language_Modeling_positiveReviews.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Afh08MKKXBJE",
    "outputId": "c725e3b3-4379-45f5-f06c-90c7352890ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad perfect esecationalism and desolated dark-comedy from early 90's and that fall in love amount of adults as well. on the only two departments of the film could be a big fan of the happy ending only it \n"
     ]
    }
   ],
   "source": [
    "#Generating text from the loaded model\n",
    "print(generate_text(classifierLoad, start_string=u\"bad \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P8EzNLLOA8m7"
   },
   "outputs": [],
   "source": [
    "################## Statistical model  ##############\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm import MLE\n",
    "from nltk import word_tokenize\n",
    "# we need to download a special component that is used by the tokenizer below -- don't worry about it. \n",
    "nltk.download('punkt')\n",
    "!pip install -U nltk==3.4\n",
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HdvY67WlcU42"
   },
   "outputs": [],
   "source": [
    "#Preparing the Training testing set with 8:2 split\n",
    "review_col = X_Y_full['review'].str.lower()\n",
    "review = np.asarray(review_col)\n",
    "!pip install pandasql\n",
    "import pandasql as ps\n",
    "review_alone = ps.sqldf('select review from X_Y_full')\n",
    "print(len(review_alone))\n",
    "train,test = np.split(review_alone.sample(frac=1), [int(.8*len(X_Y_full))])\n",
    "#Maintain a shorter length on test data for testing perplexity\n",
    "test = ps.sqldf('select substr(review,1,100) as review from  test limit 100')\n",
    "review = train['review'].to_list()\n",
    "\n",
    "#Tokenise the sentences\n",
    "texts = []\n",
    "for s in review:\n",
    "    print(s)\n",
    "    texts.append(word_tokenize(s))\n",
    "print(list(ngrams(texts[0], n=2)))\n",
    "\n",
    "#Preparing ngrams\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "train, vocab = padded_everygram_pipeline(3, texts) #### played with 3 and 2 values\n",
    "\n",
    "#model for trigrams\n",
    "model = MLE(3) # changed to 2 and experimented\n",
    "model.fit(train, vocab)\n",
    "\n",
    "#statistical test\n",
    "#sentence generation examples, played eith different lengths\n",
    "word_list = model.generate(15, random_seed=50)\n",
    "#score and counts\n",
    "print(model.counts['bad'])\n",
    "print(model.score('bad','movie is'.split()))\n",
    "print(model.counts[['bad']]['movie'])\n",
    "\n",
    "#computes perplexity of the unigram model on a testset  \n",
    "def perplexity(testset, model,ngrams):\n",
    "        testset = testset['review']\n",
    "        tokenized_text = list(map(str.lower, nltk.tokenize.word_tokenize(testset))) \n",
    "        test_data = nltk.ngrams(tokenized_text, n=ngrams,  pad_right=False, pad_left=False, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\")\n",
    "        print(test_data)\n",
    "        perplexity = 1\n",
    "        #global N;\n",
    "        \n",
    "       # for ngram in test_data])\n",
    "        #print(test_data[-1])\n",
    "        temp=1\n",
    "        N=0\n",
    "        prob=1\n",
    "        for ngram in test_data:\n",
    "            print (\"MLE Estimates:\", ((ngram[-1], ngram[:-1]),model.score(ngram[-1], ngram[:-1])))\n",
    "            #global N;\n",
    "            N += 1\n",
    "            str1=' '\n",
    "            follow = str1.join(ngram[:-1])\n",
    "            #print(follow)\n",
    "            #print(model.score(ngram[-1], follow.split()))\n",
    "            if(model.score(ngram[-1], follow.split())>0 and model.score(ngram[-1], follow.split())!=0.0):\n",
    "                prob = prob * (model.score(ngram[-1], follow.split()))\n",
    "                print(prob)   \n",
    "            else:\n",
    "                prob = prob * 0.00001\n",
    "            temp=N \n",
    "        multi = 1/prob    \n",
    "        perplexity = pow(multi, 1/temp) \n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bQzpgJzQC543"
   },
   "outputs": [],
   "source": [
    "#perplexity test\n",
    "testset1 = \"does this film suck\" #sample\n",
    "ngrams=3 # tested for 2\n",
    "# adding a perplexity column with values to test set to review later\n",
    "test['perplexity'] = test.apply (lambda row: perplexity(row, model,ngrams),axis=1)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xOLo3hCs42CH"
   },
   "outputs": [],
   "source": [
    "######################## Word Embedding ##########################\n",
    "extract = 0.2\n",
    "#text = text.lower()\n",
    "text = positive[:int(extract*len(positive))]\n",
    "vocab_words = []\n",
    "words = text.split(\" \")\n",
    "for j in words:\n",
    "        vocab_words.append(j.lower())\n",
    "        \n",
    "vocab_unique_words = sorted(set(vocab_words))\n",
    "print(len(vocab_unique_words))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OJgsiRKlwdxE"
   },
   "outputs": [],
   "source": [
    "# Create mappings between vocab and numeric indices\n",
    "char2idx_word = {u:i for i, u in enumerate(vocab_unique_words)}\n",
    "idx2char_word = np.array(vocab_unique_words)\n",
    "\n",
    "# Map all the training text over to a numeric representation\n",
    "text_as_int_word = np.array([char2idx_word[c] for c in text.lower().split(' ')])\n",
    "#print(dict_1)\n",
    "#print(idx2char_dict)\n",
    "#print(len(text_as_int_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "whXkrXqfLZa1",
    "outputId": "6834477d-f67a-4509-8206-5ca478f4c1d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38584\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length_word = 20\n",
    "examples_per_epoch_word = len(text.split(\" \"))//(seq_length_word+1)\n",
    "print(examples_per_epoch_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YF9xmi48LaOr"
   },
   "outputs": [],
   "source": [
    "# Create training examples / targets\n",
    "char_dataset_word = tf.data.Dataset.from_tensor_slices(text_as_int_word)\n",
    "sequences_word = char_dataset_word.batch(seq_length_word+1, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "rj0efHksLaxD",
    "outputId": "27cb08f3-d4a5-4be1-f360-dcc74105874d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice:0\", shape=(20,), dtype=int64)\n",
      "<MapDataset shapes: ((20,), (20,)), types: (tf.int64, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "#Dataset Preparation\n",
    "def split_input_target_word(chunk):\n",
    "    print(chunk[:-1])\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset_word = sequences_word.map(split_input_target_word)\n",
    "print(dataset_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cZeLbbuhW0u4"
   },
   "outputs": [],
   "source": [
    "#Model Design\n",
    "BATCH_SIZE = 1000\n",
    "BUFFER_SIZE = 10000\n",
    "dataset_word = dataset_word.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "embedding_dim = 256\n",
    "rnn_units = 2048\n",
    "m2 = tf.keras.Sequential()\n",
    "m2.add(tf.keras.layers.Embedding(len(vocab_unique_words), embedding_dim,\n",
    "                          batch_input_shape=[BATCH_SIZE, None]))\n",
    "m2.add(tf.keras.layers.LSTM(rnn_units,\n",
    "                    return_sequences=True,\n",
    "                    stateful=True,\n",
    "                    recurrent_initializer='glorot_uniform'))\n",
    "m2.add(tf.keras.layers.Dense(len(vocab_unique_words)))\n",
    "\n",
    "m2.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gEFy6a7ZYElc",
    "outputId": "8b0a482f-f69a-4d2e-fc75-df8bb020d5bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory not used yet.\n"
     ]
    }
   ],
   "source": [
    "#Loss function\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "m2.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = '/content/drive/My Drive/check_lstm_LM_words'\n",
    "import shutil\n",
    "try:\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "except:\n",
    "    print(\"directory not used yet.\")\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_\")\n",
    "\n",
    "cb=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    monitor='loss',\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7LPclSlFi1T0"
   },
   "outputs": [],
   "source": [
    "#Training word embeddings model\n",
    "history = m2.fit(dataset_word, epochs=2,callbacks=[cb],verbose=1)\n",
    "print(\"training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fuP74oPLBMmn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HtqIYJR9BW5H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y72WqwseBvyH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qbLhr0yDB_rl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XxbsnF7JCOQp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o_pQIXlpCfb4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
