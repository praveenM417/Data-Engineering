{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### PART 2 - Transfer Learning #########################\n",
    "\n",
    "import pandasql as ps\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dataset, shape it, extract input and target from it\n",
    "X_Y_new = pd.read_csv('owndataset.csv')\n",
    "X_Y_new['reviews']=X_Y_new['reviews'].str.replace('<br />', '').replace('[0-9]', '')\n",
    "train, validate= np.split(X_Y_new.sample(frac=1), [int(.7*len(X_Y_new))])\n",
    "x_train = (train.reviews) \n",
    "y_train= (train.sentiment)\n",
    "x_validate = (validate.reviews)\n",
    "y_validate = (validate.sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-b260c576d97f>\", line 2, in <module>\n",
      "    from keras.preprocessing.text import text_to_word_sequence\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\__init__.py\", line 3, in <module>\n",
      "    from . import utils\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\n",
      "    from . import conv_utils\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\n",
      "    from .. import backend as K\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\n",
      "    from .load_backend import epsilon\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\n",
      "    from .tensorflow_backend import *\n",
      "  File \"C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 6, in <module>\n",
      "    from tensorflow.python.eager import context\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\context.py\", line 29, in <module>\n",
      "    from tensorflow.core.protobuf import config_pb2\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 959, in _find_and_load_unlocked\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\n",
      "    from . _api.v2 import audio\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\n",
      "    from tensorflow.python.ops.gen_audio_ops import decode_wav\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n",
      "  File \"<frozen importlib._bootstrap>\", line 1032, in _handle_fromlist\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 39, in <module>\n",
      "    data = pd.read_csv(\"/home/pmohan/olympus/Customer_Match_Code_OLYMPUS_0003_part_00\",sep = '|',names=header)#print(data)\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 685, in parser_f\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 457, in _read\n",
      "    parser = TextFileReader(fp_or_buf, **kwds)\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 895, in __init__\n",
      "    self._make_engine(self.engine)\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 1135, in _make_engine\n",
      "    self._engine = CParserWrapper(self.f, **self.options)\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 1917, in __init__\n",
      "    self._reader = parsers.TextReader(src, **kwds)\n",
      "  File \"pandas\\_libs\\parsers.pyx\", line 382, in pandas._libs.parsers.TextReader.__cinit__\n",
      "  File \"pandas\\_libs\\parsers.pyx\", line 689, in pandas._libs.parsers.TextReader._setup_parser_source\n",
      "FileNotFoundError: [Errno 2] File b'/home/pmohan/olympus/Customer_Match_Code_OLYMPUS_0003_part_00' does not exist: b'/home/pmohan/olympus/Customer_Match_Code_OLYMPUS_0003_part_00'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'FileNotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 42, in <module>\n",
      "    from . _api.v2 import audio\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\_api\\v2\\audio\\__init__.py\", line 10, in <module>\n",
      "    from tensorflow.python.ops.gen_audio_ops import decode_wav\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_audio_ops.py\", line 9, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\n",
      "  File \"<frozen importlib._bootstrap>\", line 1032, in _handle_fromlist\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 39, in <module>\n",
      "    data = pd.read_csv(\"/home/pmohan/olympus/Customer_Match_Code_OLYMPUS_0003_part_00\",sep = '|',names=header)#print(data)\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 685, in parser_f\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 457, in _read\n",
      "    parser = TextFileReader(fp_or_buf, **kwds)\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 895, in __init__\n",
      "    self._make_engine(self.engine)\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 1135, in _make_engine\n",
      "    self._engine = CParserWrapper(self.f, **self.options)\n",
      "  File \"C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 1917, in __init__\n",
      "    self._reader = parsers.TextReader(src, **kwds)\n",
      "  File \"pandas\\_libs\\parsers.pyx\", line 382, in pandas._libs.parsers.TextReader.__cinit__\n",
      "  File \"pandas\\_libs\\parsers.pyx\", line 689, in pandas._libs.parsers.TextReader._setup_parser_source\n",
      "FileNotFoundError: [Errno 2] File b'/home/pmohan/olympus/Customer_Match_Code_OLYMPUS_0003_part_00' does not exist: b'/home/pmohan/olympus/Customer_Match_Code_OLYMPUS_0003_part_00'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/home/pmohan/olympus/Customer_Match_Code_OLYMPUS_0003_part_00' does not exist: b'/home/pmohan/olympus/Customer_Match_Code_OLYMPUS_0003_part_00'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Tokenise the input sequences\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=1000,filters='![0-9]\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=False)\n",
    "tokenizer.fit_on_texts(X_Y_new['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decide the vocab length\n",
    "unique_vocab = len(tokenizer.word_index)\n",
    "unique_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "#MAximum length of each sequence\n",
    "maxLen = len(max(X_Y_new.reviews, key=len).split(' '))\n",
    "print(maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 263, 40, 264, 265, 2, 266, 4, 267, 1, 2, 268, 269, 73, 2, 96, 4, 2, 6, 5, 94, 270]\n"
     ]
    }
   ],
   "source": [
    "#Tokenise Training sequences\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "print(x_train[0])\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenise validation sequences\n",
    "x_validate = tokenizer.texts_to_sequences(x_validate)\n",
    "x_validate = sequence.pad_sequences(x_validate, maxlen=maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare target labels\n",
    "y_train_new = ps.sqldf(\"select case when sentiment='positive' then 1 else 0 end as sentiment_new from train\")\n",
    "y_validation_new = ps.sqldf(\"select case when sentiment='positive' then 1 else 0 end as sentiment_new from validate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert into numpy arrays for fast processing\n",
    "y_train_new = np.asarray(y_train_new['sentiment_new'])\n",
    "y_validation_new = np.asarray(y_validation_new['sentiment_new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needed modules\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Embedding, Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 44, 50)            25550     \n",
      "_________________________________________________________________\n",
      "lstm_27 (LSTM)               (None, 44, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_28 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 176,551\n",
      "Trainable params: 176,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model design to math the previous best architecture (2 LSTM)\n",
    "model = Sequential()\n",
    "embedding_size=50\n",
    "model.add(Embedding(unique_vocab,\n",
    "                    embedding_size,\n",
    "                    input_length=maxLen))\n",
    "\n",
    "model.add(tf.keras.layers.LSTM(100,return_sequences=True,  stateful=False,recurrent_initializer='glorot_uniform'))\n",
    "model.add(tf.keras.layers.LSTM(100,return_sequences=False, stateful=False,recurrent_initializer='glorot_uniform'))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 44, 50)            25550     \n",
      "_________________________________________________________________\n",
      "lstm_27 (LSTM)               (None, 44, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_28 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 200)               400       \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 217,352\n",
      "Trainable params: 217,352\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Loading weights from previous best 2 lstm moidel\n",
    "#the below line alone will be commented in case of building a scratch model\n",
    "model.load_weights(tf.train.latest_checkpoint('bestt_2_1_lstm2'))\n",
    "#Extra fine tuning\n",
    "model.add(Dense(200,activation='relu'))\n",
    "model.add(Dense(200,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss Function\n",
    "from tensorflow.keras import optimizers\n",
    "AD = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(optimizer=AD,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 1s 31ms/sample - loss: 1.7541 - acc: 0.6111\n",
      "41/41 [==============================] - 1s 30ms/sample - loss: 0.0989 - acc: 0.9756\n"
     ]
    }
   ],
   "source": [
    "#Model Evaluation\n",
    "loss, acc = model.evaluate(x_validate, y_validation_new,batch_size=1)\n",
    "loss, acc = model.evaluate(x_train, y_train_new,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply Early stopping # experimented with and without this\n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model training and saving it\n",
    "hist = model.fit(x_train, y_train_new,\n",
    "         \n",
    "          epochs=100,verbose=1,validation_data=(x_validate, y_validation_new)\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model saving\n",
    "model.save('transfer_Learning.h5')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
